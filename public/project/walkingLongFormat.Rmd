---
title: "Stream Occurence"
author: |
  | Josh Erickson
  | USDA Forest Service, Northern Region, Kootenai National Forest
  | joshua.l.erickson@usda.gov
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
bibliography: walking.bib
---

```{r setup, include=FALSE}

load("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/workingthroughproj.RData")
options(width = 200)
library(sp)
library(rgdal)
library(rgeos)
library(raster)
library(automap)
library(caret)
library(CAST)
library(blockCV)
library(tidyverse)
library(gridExtra)
library(reshape2)
library(doParallel)
library(parallel)
library(mlbench)
library(plotROC)
library(pdp)
library(vip)
library(gridGraphics)
library(grid)
```


# Introduction  

**Introduction**  

This is a workflow used to create the probabalistic spatial prediction map of stream occurence in northwest Montana.  Hopefully by providing this workflow we can improve this analysis. The reason for such a dense, superfluous, and tedious workflow is that I like doing this type of analysis and also it helps me keep everything in perspective. However, it should be noted that this not the only way or is in any way intended to be a 'cookbook' so please take this into consideration. There are many, many ways and methods for this type analysis and a lot of it depends on the project details and objectives. Thus I want to start by highlighting some goals of this analysis and workflow, which may help summarise my rationale.  

1. Balancing prediction performance with model interpretability while emphasising a spatial modeling framework.  

2. Use feature selection techniques that will lower model processing time while reducing variance (i.e. accounting for spatial autocorrelation).  
3. Optimizing climate, topographic, and remote sensing data to better predict stream occurence and understand hydrologic processess at multiple scales.  

4. Test new spatial prediction techniques (forward feature selection, blockCV) with others (recursive feature elimination) so that future endeavors can be efficient while considering limitations.  

**Please use the tabs to navigate through this workflow process. Here is a brief summary of each tab.**  

* **Accessing Data** - If you want to get your hands on the data go here.   

* **Methods** - how we got to modeling:  

  + Variable(s) - summary of predictor and response variable(s).  
  + Pre-processing - examining predictor variables.  
  + Data Splitting Guidance - determining how to split the data based on:  
  
      - Proportion - what's the response's proportion?    
      - Balanced? - to resample or not?  
      - Relevant Variables.
      - Prevalance - sample prevalance rate? 
      - Data Leakage.
      - Dependence Strutures (i.e. spatial autocorrelation). 
      
  + Evaluation Techniques - feature selection and tuning/evaluation techniques  
  
      - Feature Selection - summary of different techniques.  
      - Tuning/Evaluation - summary of different techniques.  
      
* **Modeling** - the actual modeling:  

  + Model Assessment - the actual modeling with training/evaluation data.  
  + Model Selection - selecting the best model while considering goals.  
  + Model Performance - testing the final selected model on the test data.    
  
* **Discussion** - final thoughts on results.  

* **References** - bibliography
    

Thank you for your time and critiques as this will only make it better. Please, any comments add to the document or email [joshua.l.erickson@usda.gov].  

# Accessing Data  

**Accessing Data**  

Real quick, the csv data can be downloaded at [csv](https://drive.google.com/open?id=1Vich0zlZSmRHFM7cCLyziUmmzqN6f6Pb), the points downloaded at [points](https://drive.google.com/open?id=1CvRYDRJ1xPWZ9EzdByye2I9gKSw9Ljlu), and the tifs downloaded at [TIF](https://drive.google.com/open?id=100in8JlxDCbRLTPCzQ-p9D54SQxIP37d). The **.tif files** have been pre-processed so that projections and datums are the same and the csv data is raw so that the user can explore if they want to improve with other models or techniques (eg, cv methods, sampling, models). The points are provided to help show where the data was collected or if wanting to do a 'ground up' analysis as well. The point samples were collected from past and current surveys using the Montana "SMZ law" definitions, which you can find here [SMZ, page 3](http://dnrc.mt.gov/divisions/forestry/docs/assistance/practices/smzfullcopy.pdf) and also described in [Variables]. 

# Methods 

## Summary  

**Summary**

This analysis was built off a lot of research done on the relationships between climate data, topographic indices, and hydrologic processes [@beven1979physically; @dobrowski2013climate; @hird2017google; @HoldenPrep; @hoylman2018hillslope;@hoylman2019climatic;@jaeger2019probability;@lidberg2020using;@pelletier2018way; @radula2018topographic;@robinson2018terrestrial;@sando2015predicting;@sorensen2006calculation;@stephenson1998actual]. The methodology, concepts, and workflow for the analysis were adapted from [@friedman2001elements;@james2013introduction;@kuhn2013applied;@hird2017google;@jaeger2019probability;@meyer2018improving;@meyer2019importance] where the authors used machine learning techniques to predict different types of response variables. Variable categories will follow three different types (e.g., digital elevation models (DEM), radar images, and optical images) that @hird2017google used for predicting "wet" and "dry" areas in northern Alberta, Canada. This is also a common variable selection approach as others have done [@jaeger2019probability;@lidberg2020using;@sando2015predicting] within a hydrologic context.   

Within the methods section, the goal is to emphasize the importance of accounting for spatial autocorrelation in regards to statistical learning validation techniques. It should be noted that autocorrelation in statistical learning will invalidate some of the assumptions that regression and parametric type models must have (e.g. independence) ; however, with ensemble modeling these assumptions are not neccessarily as important. But, ensemble modeling doesn't get a free lunch and spatial autocorrelation affects the validation techniques as well as feature selection [@meyer2018improving;@meyer2019importance;@roberts2017cross;@valavi2018blockcv]. Spatial autocorrelation affects all models in this way (e.g. validation is over-optimistic); however, it may be even more severe with ensemble modeling due to the propensity to overfit in the first place, i.e. flexible models typically have low bias and high variance which can overfit (high variance) more than a less flexible model. Also, I will bring up the concern for using *artificially* balanced data sets. 

This has been echoed by numerous authors [@kuhn2013applied; @matloff2017statistical; @meyer2018improving;@meyer2019importance;@roberts2017cross;@valavi2018blockcv], which have provided different approaches when dealing with these concerns (e.g. validation techniques, prevalence rates). However a feature selection process is still imperative within a spatial context as @meyer2018improving;@meyer2019importance has shown and should be investigated as well so the model can lessen any sneaky surrogate or geo-located type variables that consistently overfit and plague spatial prediction covariates. This coupling of overfitting (e.g. validation and prevalence methods) leads to over-optimistic results and can significantly affect extrapolation and interpolation [@matloff2017statistical; @meyer2018improving;@meyer2019importance;@roberts2017cross]. I will take these approaches and apply them to the data collected in northwest Montana along with some traditional methods of machine learning workflows, e.g. data splitting, leakage, etc. This may seem stringent at times but the goal is to have realistic predictions, within constrained timeframes, and while being spatially responsible. Please, if any of these approaches or methods are confusing or contradictory let me know and I would be glad to rethink my approach and respond. Thanks.  




## Variables  


**Predictor Variables**  
 
Digital elevation models (DEM) were used to generate the following: topographic wetness index (TWI), topographic position index (TPI), Deficit continuos parameter grids (CPG), and upslope accumulated area (UAA). The methods used to derive these indices followed @tarboton2013taudem by using the command-line function in R [@team2014language]. A 10-m DEM USGS National Elevation Dataset [@gesch2009national] was used to calculate upslope accumulated area (UAA) by using the d-infinity algorithm [@tarboton2013taudem]. Topographic wetness index was then calculated at 10-m resolution by using UAA and slope. Both TWI and UAA were then aggregated to 30-m resolutions with a 3x3 mean window function and resampled with nearest neighbor to retain the intricacies of the flow path direction and be able to  stack with 30-m optical and radar images. Topographic position index (TPI) used the 10-m DEM USGS National Elevation Dataset and was also aggregated to 30-m resolution using a 3x3 mean window function and resampled with nearest neighbor. The PRISM CPG was taken from @sando2018cpg where the variable is created by accumlating a weighted grid (average annual PRISM data (1981-2010)) and normalizing it by the UAA, which gives a ratio of precipitation and accumulated area. For the deficit CPG, the indice was derived from the 10-m flow direction grid and then aggregated to 30-m flow direction grid (to retain intricacies) from the same processeses as above [@tarboton2013taudem], from which the grid was then weighted by a CWD grid (i.e. 30-m). This was then normalized by the UAA 30-m grid. The reason to add climatic water deficit (CWD) the observed interplay between these processes in the field. 

Radar image collections included 10-m Sentinel-1 polarimetric Synthetic Aperture Radar (SAR) images from ('05-15' to  '08-31') for each year (2015-2019) following the methods described in @hird2017google: Normalized Polarization (Pol), Vertical Polarization (VV), VV Standard Deviation (VVsd). These images were collected using Google Earth Engine (GEE) [@gorelick2017google] and code by [@S1hird2017google]. These were then aggregated to 30-m resolutions using a 3x3 mean window function and resampled with nearest neighbor.  

Optical image collections included 10-m Sentinel-2 optical satellite images from August-September ('08-01' to '09-30') for each year (2015-2019) to produce Normalized Difference Vegetation Index (NDVI) and Normalized Difference Water Index (NDWI). In addition to NDVI/NDWI, the 10-m Sentinel-2 optical satellite Blue (B2), Green (B3), Red (B4), Near Infrared (B8) images from   August-September ('08-01' to '09-30') for each year (2015-2019) were also used. These were again aggregated to 30-m resolutions using a 3x3 mean window function and resampled with nearest neighbor. Median Net Primary Productivity (NPP) [@robinson2018terrestrial] from 1986-2018 and annual 30-m Climatic Water Deficit (CWD) [@HoldenPrep] from 1986-2016 were also used as predictors. Normalized Difference Vegetation Index (NDVI), Normalized Difference Water Index (NDWI), and median Net Primary Productivity (NPP) were collected using GEE. The 30-year Annual (July-Sept; 1986-2016) Normalized Difference Vegetation Index (NDVI), annual Climatic Water Deficit (CWD), Cold Air Drainage (CAD), Decidous (Decid), and Hydrometeorlogical Dryness Index (HDI) were taken from @HoldenPrep.  

**Now bring in all the TIFs to be used in the anlaysis and stack/visualise.**
```{r, eval = FALSE, class.output = "pre"}
library(raster)

twi30 <- raster("twi30agg.tif") #TWI

vvsd30 <- raster("vvsd30agg.tif") #vertical vertical sd

vv30 <- raster("vv30agg.tif") #vertical vertical mean

npol30 <- raster("npol30agg.tif") #normalized polarization 

ndviAS30 <- raster("ndvias30agg.tif") #NDVI aug-sept

ndwiAS30 <- raster("ndwias30agg.tif") #NDWI aug-sept

accum30 <- raster("accum30.tif") #UAA d-infinity aggregated from 10-m

nppMid30 <- raster("nppmmid30agg.tif") #NPP median '86-18'

ndvi30yr <- raster("ndvi30yrRS.tif") #NDVI 30 yr

deficit <- raster("deficitRS.tif") # annual CWD 30 yr

wtrbdy30 <- raster("wtrbdy30agg.tif") #waterbodies

tpi30 <- raster("tpi30agg.tif") #TPI

HDI <- raster("hdi30RS.tif") #hydrologic deficit index

CAD <- raster("cad30RS.tif") #cold air drainage

decid <- raster("decid30RS.tif") #deciduous

B2 <- raster("B2_30agg.tif") #blue (B2)

B3 <- raster("B3_30agg.tif") #green (B3)

B4 <- raster("B4_30agg.tif") #red (B4)

B8 <- raster("B8_30agg.tif") #Near Infrared (B8)

cpgPrecip <- raster("cpg30precip.tif") #Continuous parameter grid (precipitation/PRISM).

cpgDeficit <- raster("cpg30Deficit.tif") #continuous parameter grid (deficit).

topo_opt_rad30 <- stack(twi30, tpi30, accum30,vv30, vvsd30, npol30, ndvi30yr, ndviAS30, ndwiAS30, nppMid30, deficit, HDI, CAD, decid, B2, B3, B4, B8, cpgPrecip, cpgDeficit)
```

<center>
```{r, eval=TRUE}

plot(topo_opt_rad30, maxnl=32, nc = 4, legend = FALSE)

```

</center>
<br>


**Response Variable** 

The response variable was collected in a few different ways: historic and current data observations. The historic observations were collected from 1992-2004 by US Forest Service personel. They would walk drainages and identify streams by using stream classification protocols at the time (recommend **copy link address**) ([SMZ, page 3](http://dnrc.mt.gov/divisions/forestry/docs/assistance/practices/smzfullcopy.pdf), [Kootenai National Forest stream classifications](https://drive.google.com/open?id=1lX1HDkZwCiXFioaVYy4s216bunKX2d8C)).  

These were then evaluated using current data observations in places where these methods overlapped. Also, points were applied to the surveyed data at a 30 meter distance. In adddtion, areas with past timber sales were used as places to interpret “no stream.” Now there are a lot of assumptions with this type of data collection (observer bias, overlooked areas, streams change, etc); however, by observing these areas with the currect data observtions it was validated in an unsupervised manner (using current data, old maps, professional judgement, remote sensing, etc) with high agreement.   

The data that overlapped showed strong similarities in the results, which made me more confident in using these data collection methods. The current data collection was performed from June-Oct in 2019. These collection methods followed the same protocol as the SMZ method. Data was collected with a Samsung Galaxy Tab A Tablet with +/- 15-30 foot accuracy. This was another reason to only use 30-meter resolution grids in the analysis as well; the accuacy at which the data was collected would have been suspect at 10 meters. The current data collected only accounted for appoximently 1500 points but was very helpful in validating the historic data as well. Below shows how the raster stack of all the predictor variables were extracted by point samples.   

**Extract points from stack `topo_opt_rad30` and then combined with `pts`.**
```{r, eval = TRUE}

library(arcgisbinding)
arc.check_product()

#combine point objects
pts <- arc.open("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/points_spaced30m.shp")  #points to the folder/feature
pts <- arc.select(pts, c("copy_TWI_1", "FID_ksank1", "FID_ksan_1")) #selects the data in the folder

                        #copy_TWI_1 = stream or no stream, FID_ksan1 & FID_ksan_1 are just the
                          #HUC 12 and HUC 14 polygon attributes
                            #we will use later for leave location out (LLO).

pts <- arc.data2sp(pts) #now we have a spatial points data frame
```
```{r, eval=FALSE}
library(sp)
library(rgdal)
library(rgeos)

deficit <- raster("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/deficitRS.tif") # annual CWD 30 yr

thrty.3 <- mask(topo_opt_rad30, wtrbdy30, inverse = TRUE) #mask out waterbodies if you want

#Get forest service boundary
FSland <- readOGR("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/KNF_ownership.shp")
FSland <- FSland[FSland$OWNER == 'FS',]

#Get District Boundary
DistrictBoundary <- readOGR("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/fortine_rexford_RD_Bdy.shp")

#now intersect and project
landClip <- gIntersection(FSland, DistrictBoundary, byid = TRUE)

tpiCRS <- crs(tpi30)

landClip <- spTransform(landClip, tpiCRS)


thrty.3 <- mask(thrty.3, landClip) #mask out non-Forest Service land

thrty.3 <- raster::extract(thrty.3, pts) #extract the values from raster by point data

top30.opt.rad <- cbind(pts, thrty.3)
top30.opt.rad <- data.frame(top30.opt.rad)
plot(thrty.3)

write.csv(top30.opt.rad, file = "D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/top30.opt.rad.csv")
```





## Pre-processing 

**Pre-processing**  

Pre-processing data is an important step in any data anaysis. With pre-processing we will look at how the data is distrubuted (shape) and correlated. Typically within environmental science data, transformations are needed since the data normally follows a skewed pattern (i.e. lots of small events with periodic high events or vice versa) [@stat2011thumb]. However, we will be using tree-based *classification* methods (e.g., RandomForest, Gradient Boost) for the predictive model; thus, transformations will not be as important because tree-based *classification* models are not sensitive to scale or distributions (i.e., metrics used for partitioning nodes don't involve regression-type statistics (RSE) instead splits feature space(s) by purity methods, e.g. Gini index, cross entropy). However, just like any statistical learning practice, context matters. Thus you may want to transform your data, even with a tree-based *classification* method. That being said, I still think it is prudent to look at the data descriptively before we jump in regardless of model type.  

To start we can look at the shape of the data and how it's plotted against other variables. This involves looking at histograms and scatter plots of the data visually. Since we have 19 predictors this will be pretty difficult to look at scatter plots so we will just look at histograms (sidebar - I've look scatter plots individually and there are some interesting trends but mostly uncorrelated). Let's bring in the data we just created in the Variables section and start exploring.

```{r, eval=FALSE}
top30.opt.rad <- read.csv("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/top30.opt.rad.csv", header = TRUE)[-1]

data <- top30.opt.rad #change for ease
data <- data[,-26] #take out irrelavant index
#change names
names(data)[2] <- "HUC12"
names(data)[3] <- "HUC14"
names(data)[1] <- "stream"
data$stream <- factor(data$stream)
data <- na.omit(data)
 # write csv

write.csv(data, file = "D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/trainDat.csv")

```
<br>

Now lets look at the histograms of the predictor variables. 
<br>

```{r fig.align='center',message=FALSE, echo = FALSE}
library(tidyverse)
top30.opt.rad <-  na.omit(top30.opt.rad)
ggplot2::ggplot(gather(top30.opt.rad[,c(4:23)]), aes(value)) + 
    geom_histogram(bins = 20) + 
    facet_wrap(~key, scales = 'free_x')

```


<br>
From the graph above it looks like there are some outliers in the data set. However, the tree-based models we will use are not so sensitive to outliers; again, it should be noted that you don't neccessarily want to remove outliers so hastily and understanding of the data is key. No free lunch! On the other hand, correlation statistics can be sensitive to outliers and tree-based *classification* models can have a hard time with correlated variables. So we will want to explore any noticeable outliers before doing any correlation estimates. I'll just use a simple outlier detection by looking at the histograms. From the histogram there were three graphs that looked suspect: npol30agg, hdi30RS, and accum30 so we will look at those.


```{r echo=FALSE, fig.align='center'}

par(mfrow=c(1,3))
boxplot(top30.opt.rad$npol30agg, main="Normalized Polarization")
boxplot(data$hdi30RS, main="HDI")
boxplot(data$accum30, main="Upslope Accumulated Area")





```
<br>
Both `npol30agg` and `accum30` look like they have a couple of significant outliers. Thus leaving `accum30` alone will be fine (since an UAA that large is likely) but the npol30agg needs to be between -1 and 1, theorhetically. This is totally appropriate because it doesn't affect the 'true' distribution of the data. For example, if we took outliers out that were theorhetically possible, then the test data would have a peak at the 'new' distribution, i.e. data leakage. We'll talk abou data leakage later but remember to not hastily take out data without taking leakage into consideration. 

```{r eval=FALSE}
data <- data[!data$npol30agg < -1,]
data <- data[!data$npol30agg > 1,]
```
<br>
Now look at the new boxplot and histogram for `npol30agg`, which looks much better.

```{r echo=FALSE, fig.align='center'}
#now look at boxplot
par(mfrow=c(1,2))
boxplot(data$npol30agg, main="Normalized Polarization")
hist(data$npol30agg, main = "Normalized Polarization")

#much better
```


The next step is to look for correlated data. Correlations in the data will give us an idea of some redundant data (e.g. collinearity). We can investigate correlation visually through the function `corrplot` and look for any colinearity with the predictors or see if there is any clustering `hclust`. These type of plots are very useful when the amount of predictors is above 20 (`pairs.plot` gets overwhelmed) and gives a quick view of what's going on. After visualising, we can take a deeper look and search the data frame for correlation using a certain threshold (e.g., 90%). This is possible by  `caret`'s function called `findCorrelation`. This is the cutoff threshold that @hird2017google and @lidberg2020using used but it might be worth exploring some lower cutoffs. I tried 60% to start off and got eight variables.

```{r fig.align='center'}
library(corrplot)
library(caret)
correlations <- cor(data[,-c(1,2,3,24,25)]) #remove response,coords and HUCs
corrplot(correlations, order = "hclust")

highCorr <- findCorrelation(correlations, cutoff = 0.6)
highCorr
```



There are eight variables recommended to be taken out. Let's see what they are. 

```{r}
colnames(correlations)[c(16,15,20,11,12,18,8,4)]
```

There are a few ways we can approach this; take them out, do some sort of dimension reduction or raise the threshold bar. We can check the threshold by moving it up to let's say 75%, which is a little more of a conservative value. 

```{r}
highCorr2 <- findCorrelation(correlations, cutoff = 0.75)
highCorr2
colnames(correlations)[c(16,15,20,11,18,8)]
```

With this threshold of 75% we ended up only dropping `hdi30RS` and `vv30agg` in the list. The remaining variables make sense why they would be correlated; `cpg30Deficit` and  `deficitRS` both have CWD, the B's are all reflectances, and `ndvias30agg` is highly correlated with its counterpart `ndwias30agg` and B8. By keeping these variables in the analysis it is likely to be couterproductive unless some sort of dimension reduction is applied. The disadvantage of dimension reduction is that you lose interprebility with these variables and you also might miss the mark on causation. Advantages include using decorrelated variables that normally would have been taken out of the model and reducing dimensions in statistical learning is always a worthwhile consideration (e.g. 'curse of dimensionality'). It's a tricky decision.  

There is no clearcut way of doing this, so suggestions are welcomed. Again, no free lunch!. Thus I decided to drop `ndvias30agg` and here's why; I don't mind dropping `ndvias30agg` because we have `ndwias30agg` and we also have a long-term NDVI `ndvi30yrRS`as well. Therefore, by dropping this variable we remove some redundancy and keep some interprebility which I think is a good trade-off in this analysis. However, I decided to do a principal component analysis (PCA) with the reflectance band variables and the two CWD variables. One could probably argue to just take away one of the deficit variables but for some reason my confirmation bias want to keep them. So bear with me on the rationale below as it would be just as easy to take one out.

The reflectances interprebility to me seems hard pressed in the first place so the trade-off comes down to possible model performance over interprebility. Therefore, I would rather keep them in the analysis with the downfall of losing some interprebility at the expense of possibly having better model performance. It's a little tricky with the CWD variables, we would want some interprebility from them but we are going to have to lose that with a reduction, again trade-offs. I think that they are both below a 90% threshold, so leaving them wouldn't be the worst thing but we also have `hdi30RS` as a predictor, which is a recently studied, validated, and published model for water and energy feedbacks. Therefore reducing the dimension of `deficitRS` and `cpg30Deficit` at the expense of interprebility isn't as big of loss. However, if we didn't have `hdi30RS` I would probably consider dropping one.  

Now we want to make sure that we don't leak any data in the process (i.e. data leakage) as well so we will need to process the components during cross-validation. What this means is that you don't want to preprocess your PCA data on both test and training data; thus, a PCA prior to cross-validation is a concern and should be avoided. The PCA will be done within the cross-valdition procedure in the `caret` function using a `recipe`, which allows us to be specific with variables that we want to preProcess. So lets take out the predictor variable `ndvias30agg` that we determined to be redundant and collinear and wait to do anything with the PCA until later during the modeling. 

*Update I'm dropping `cpg30Deficit`. In short, not worth it. Will be exploring in the future though.  


```{r eval=FALSE}
data <- data[, -which(names(data) %in% c("ndvias30agg", "cpg30Deficit"))]

```


Now we can move on to the spatial pre-proccesing section.


**Spatial Pre-proccessing**  

Pre-proccessing the spatial data is a very important process (e.g., projections, resolution, etc) and can be troublesome if not done correctly or overlooked. The predictor variables mentioned in the *Variables* section used the following pre-processing techniques below. These data started at 10m resolution (e.g., tpi, twi, uaa, ndvi/ndwi Aug-Sept, vv, vv sd, npol) and were then aggregated by a mean function (3x3 window) and resampled by nearest neighbor to retain inherent values from the aggregation. The aggregated and resampled data were then rasterized and saved as a tif. Below is the process to get each TIF from 10m to 30m resolution. Resampling was done to make sure any changes in the aggregation would be rectified, if any. This pre-processing was also done to all the 30m spatial data that wasn't 10m to retain correct CRS.  

```{r, eval = FALSE}
tpi30 <- aggregate(tpi, fact = 3, fun = mean) #aggregate from 10-m TPI
tpi30 <- resample(tpi30, ndvi30yr, method = "ngb")
writeRaster(tpi30, filename = "tpi30agg.tif", overwrite = TRUE)
tpi30 <- raster("tpi30agg.tif")

```

To check the correct CRS, resolution, extent, and dimensions for all the variables I used the process below. However, I will only show the first couple of grids. The data is already pre-processed so i'll bring in a differing grid to show the process.  

```{r eval=TRUE}
library(raster)
dem30test <- raster("D:/Rcodes/FDir/dem30test.tif")
dem30test
tpi30
```

Now project `dem30test` to `tpi30` using the function `projectRaster`.
```{r eval=TRUE}
tpiCRS <- crs(tpi30)

dem30proj <- projectRaster(dem30test, tpi30, res = 30, crs = tpiCRS, method = "ngb")

dem30proj <- raster::resample(dem30proj, tpi30, method = "ngb")

writeRaster(dem30proj, "D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/dem30proj.tif", overwrite = TRUE)
```

Now the data is in the same projection, ellipsoid, datum, extent, and dimension. Do this for all the grids in the data set. The reason for albers conic equal area (`+proj=aea`) was that the DEM was in this projection and so the flow accumulation, twi, tpi were built off of these. In the end I don't think it matters too much since the sample space is relatively local. But any comments would be appreciated.
```{r}
dem30proj
tpi30
```

Heres how I derived the CPG grid for deficit below.

```{r eval=FALSE}

tpi <- raster("tpi.tif") #bring in tpi for cropping

z=raster("D:/Rcodes/FDir/dem10m.tif") #bring in 10-m DEM

c <- crop(z,tpi) #crop to tpi

writeRaster(c, "croppedDEM10.tif")

c <- resample(z, tpi, method = "ngb") #resample with tpi
c <- mask(c,tpi) #mask with tpi

writeRaster(c, "dem10m.tif", overwrite = TRUE)



#####This is the command-line Tarboton TauDEM process####

# Pitremove
system("mpiexec -n 8 pitremove -z dem10m.tif -fel dem10fel.tif")
fel=raster("dem10fel.tif")
plot(fel)
zoom(fel)

# DInf flow directions
system("mpiexec -n 8 DinfFlowdir -ang dem10ang.tif -slp dem10slp.tif -fel dem10fel.tif -nc",show.output.on.console=F,invisible=F)

dem10ang <- raster("dem10ang.tif") 

defdis <- disaggregate(deficit, fact=3) #downscale deficit 30-m raster

s <- resample(defdis, dem10ang, method = "ngb") #then resample

s <- mask(s, dem10ang) #mask to get it aligned with the angle raster

s[is.na(s[])] <- 0 #now set NA's equal to zero so TauDEM deoesn't fuss

writeRaster(s, "defdis.tif", overwrite = TRUE)

# Dinf contributing area

#weighted with deficit

system("mpiexec -n 8 AreaDinf -ang dem10ang.tif -sca DEF10sca.tif -wg defdis.tif -nc")

#normal UAA
system("mpiexec -n 8 AreaDinf -ang dem10ang.tif -sca DEM10sca.tif -nc")

#now do the math
cpg30Deficit <- overlay(DEF10sca, DEM10sca, fun = function(r1,r2){return(r1/r2)})

#then aggregate to 30-m to retain finer scale flow patterns

cpg30Deficit <- aggregate(cpg30Deficit, fact=3, fun=mean)

#then get it lined up with the other rasters

cpg30Deficit <- projectRaster(cpg30Deficit, tpi30, res = 30, crs = tpiCRS, method = "ngb")

cpg30Deficit <- raster::resample(cpg30Deficit, tpi30, method = "ngb")

writeRaster(cpg30Deficit, "cpg30Deficit.tif", overwrite = TRUE)

```


## Splitting Data 

### Guidance

**Data Splitting Guidance**  

Data splitting is a crucial part of the modeling workflow because it involves how the data gets trained/tuned and tested, which ultimately affects performance (i.e. interpolation and extrapolation) [@kuhn2013applied]. This section will have a lot of details that at times will seem tedius, redundant, stringent and nuanced. My appologies. However, I think the cost benefit of following these approaches will only help correct for overfitting and insufficient model interpretations. That is to say, the cost is low to explore these details while the benefit is high when compared to the complement, e.g. plug and chug.  

Thus different data splitting approaches can influence the risk of overfitting [@friedman2001elements;@matloff2017statistical;@james2013introduction;@kuhn2013applied] and data leakage [@kaufman2012leakage]; especially within a spatial context [@bahn2013testing;@elith2008working;@ives2006statistics;@meyer2018improving;@meyer2019importance; @roberts2017cross]. One way to pursue these splitting approaches while avoiding pitfalls can be through how you split your data and/or interpret and choose your data (predictors) i.e. *Data Splitting Guidance*. This is one of the sections in the workflow where I got into some really deep rabbit holes and went back and forth on different strategies. That be said, perpspective or comments in this section would be much appreciated. The **actual** data splitting will be done in the 'Modeling' section and think of this as the rationale of how we got there. Thus I will take these steps below, which is a blend of  @kuhn2013applied, @matloff2017statistical, @meyer2019importance, @roberts2017cross and @valavi2018blockcv as ways to approach structured and/or imbalanced/rare data in a statistical learning context. Below is an outline of this section.

* Proportion of response.
* To balance or not to balance.
* Cause and effect of variables to response.
* Prevalance. Who cares...
* Minimize data leakage.
* Assess dependence structures in data  


### Proportion 

**Proportion**  

I will start off by looking at the proportion of classes ("stream", "no stream") in the response variable. Zero (0) = "no stream" and one (1) = "stream". I used the `arcgisbinding` package but of course other methods can be used. I like interfacing between ArcGIS and R because of the visuals, however packages like `mapview` are just as good for exploring visually and are free! As always, you can bring in the `points_spaced30m.shp` with the `sf` package.

```{r, out.width= "70%", fig.align='center', message=FALSE}

barplot(prop.table(table(pts$copy_TWI_1)),
        main = "Proportion of stream occurence", 
        names.arg = c("No Stream", "Stream"), ylab = "Proportion %") 
```
```{r, eval=TRUE, echo=FALSE}
kableExtra::kable(addmargins(table(Stream = pts$copy_TWI_1)), caption = "Count")
kableExtra::kable(round(prop.table(table(Stream = pts$copy_TWI_1)),3), caption = "Proportion")
```
<br>

The proportion and count of the response brings up some important questions. Should we downsample or upsample or leave it how it is? How much data should we use for evaluating performance e.g., training/tuning? What is the prevalence of stream occurence in the first place (prior)? These are all important questions and I hope to acknowledge them below but looking for some additional insights into this process as well.  




### Balanced? 

**Balanced?**    

Within environmental classification models there can be a natural imbalance when collecting the data randomly, i.e. target class is typically rare. Downsampling or upsampling can correct for this class imbalance when training the data, which means the model doesn't have to deal with this imbalance while training. However, it's not that imbalanced data sets are a problem and must be balanced but it ultimately depends on the goals and  objectives of the project, i.e. sometimes the minority group is so rare (<1%) that balancing might help spot trends in prediction which can help with interpretation and selection. We will explore the "to balance or not to balance?" in the paragraphs below.   

Typically you want to sample randomly so that you end up with a unbiased sampling distribution in the first place. If a random sampling method was pursued, in our case, results would most likely be 1-10% proportion for the minority class (*stream occurrence*) and the rest going to *no occurrence* (>90%), depending on location. This means having really really low samples of our response target class, i.e. stream occurence! As you saw in the proportion section we have a higher proportion of streams occuring compared to streams not occurring, which as you can imagine is not a realistic distribution. This artificial distribution between classes highlights the observation bias in our sampling scheme and points to some flaws we might run into when we model.  

Consequently, collecting data this way (e.g. observation bias) lends itself to a balanced data set. This is not a bad thing but should be noted and always in the back of the head for the modeler, i.e. we have a lot of *"stream occuring"* samples. Therefore, leaving the sample proportion the way it is could potentially be adequate for our analysis. However, there are conflicting interpretations and suggetions for dealing with these imbalances [@kuhn2013applied; @matloff2017statistical]. Therefore, just remember it's biased towards the minority in the training context and it might be advantageous to train on *natural* prevalence distributions between classes.  

It does bring up a good point though e.g. prevalence, how should the test data be tested or used? This is where the importance of spatial feature and validation techniques are necessary because the data are collections of non idependent realizations in the sampling process and also have a serious imbalance (if collected randomly). Thus spatial autocorrelation is present in our sampling scheme along with a 'rare-ish' prevalance rate. I want you to just keep this topic in mind. That is to say, minority group is *stream*, majority group is *no stream* and what's the actual rate of prevalance (it's definitely not 50/50!) nested within a spatial context. This might give some insight into how to split and evaluate the data so that we can achieve optimal results while accounting for these conditions!  

Ultimately we want a model that can perform well but also be aware of what the *natural* conditions are so that we can make more informed decisions under uncertainty. Testing these two approaches (balanced vs. unbalanced/natural) will be an interesting topic and study as others have alluded to its concern [@kuhn2013applied; @matloff2017statistical;@roberts2017cross].



### Cause and effect  

**Cause and effect**  

One way to start anticipating the data splitting process is to go over predictor variables and examine whether or not these variables would be appropriate predictor variables in the first place. This is really important if the goal is extrapolation because irrelevant variables can fail big time when going beyond your 'small world' model. Surrogate variables are common traps to avoid with any model and should be met with skeptisism (i.e. spurious correlation). Typical surrogate variables in environmental sciences include but are not limited to: elevation, coordinates, slope, ids, similar ratios, dummy variables, etc [@roberts2017cross; @meyer2018improving; @meyer2019importance;@stat2011thumb]. The goal is to avoid these type of variables in the first place and/or transform appropriately; therfore, a thourough evaluation of your variables is a good idea. However there are many times where the predictor variables exceed such a value that any sane person would not dare to approach a one-by-one strategy. Hence, spatial feature selection and validation techniques [@roberts2017cross; @meyer2018improving; @meyer2019importance; @valavi2018blockcv] have been provided by these authors to lessen the strain and time of the modeller within these structured dependance domains. These feature and validation techniques are an option to search your variables if size of *p* is too demanding but it's always good to give an adequate examination if not (this is especially true if you're out of your domain e.g. hydrologist determining wildlife predictors). 

In our example there are 16 predictors that were specifically chosen from hydrologically relevant papers [@hird2017google;@hoylman2019climatic;@jaeger2019probability; @lidberg2020using;@sando2015predicting] where the response (water) and predictors (remote sensing data) were used to optimize prediction. Three other variables: `decid`, `cad`, and `deficitCPG` are new to modeling hydrologic processes, from what I could gather. Of these three variables, the one variable that I could see being spurious is `deficitCPG`. This variable will be one I keep my eye on through the modeling process. On the other hand, all the other variables we'll be using are considered relevant within a hydrological context and appropriate for cause and effect relationships, especially when considering critical zone processes [@pelletier2018way]. One concern that was brought up though was whether or not Upslope Accumulated Area (UAA) was a surrogate variable or not? This was the case for the @jaeger2019probability paper where the authors decided not to include UAA exactly for this reason. @jaeger2019probability objectives were different than ours in the sense that they were concerned with stream permanance but not with stream occurrence. There's a subtle difference here and to us it makes sense to include UAA. However, this is a good example of where data that's not relevant or causal to sneak in. 

Hence we'll leave the predictors we already selected. It would be challenging to test the UAA surrogate hypothesis. The only other option would be to see if the foward feature selection picks UAA or not? The hypothesis is that if UAA is 'geo-located/surrogate' variable then predicting outside its dependance structure space (i.e. range) on the the response would by theory result in the forward feature selection from @meyer2018improving to not pick it as a variable or it's highly unlikely to pick it. This is due to these type-variables propensity to under-and-over fit, i.e. very high error with model performance metrics, see @meyer2018improving. Therfore, this will be a topic to address in the anlaysis results/discussion section. If there are other ways to possibly test for this particular issue please let me know. 



### Prevalance 

**Prevalance**  

Please read this with the caveat that it's a interesting topic but is very nuanced and heavy on theory. I can sum it up pretty quick; do we use *natural* prevalence rates or not? To balance these two methods I propose using minority prevalance rates $(0.05, 0.10, \dots, 0.50)$ to test the final models on and using minority prevalence to compare against a balanced data set. For example, after tuning the models we will have a good idea of what the models perfomance will be over multiple tuning parameters (this is for the balanced data set). Then we can use the final test data to compare our tuned models using the different minority thresholds, e.g. let's say we have 2000 in the final test set; then for each model, $2000*(0.05, 0.10, \dots, 0.50) = (100,200, \dots , 1000)$ we will have a *minority* amount we can test the model on. This seems like a nice idea but to acutally test it we would need to model a seperate set of data with *natural* prevalence rates.   

The reason why this is so important is that if you train your model with a *artificial* proportion, then your model will assume a 50/50 split in the *real world* as your *un*conditional probability. In our case, this could lead to overfitting because when the model starts to predict (unknown locations) the model assumes a *un*conditional probability of 50%, which effects the feature splitting (e.g. mtry or root node and Gini Index and loss function in our case) and the logistic function in GBM, i.e. GBM uses pseudo residuals to slowly grow learners, however it starts with an initial probability and that's where we run into a problem because it assumes that both classes are 50/50 or whatever the class proportion is (55% in our case), which is not correct and ultimately affects the residuals. Again, this is very nuanced and the level to which this will affect the model in our case is unknown but it is also the reason we want to test for it.  Below shows a rfe model with 55% "stream" and 10% "stream". Notice the big difference between the spec/sens relationship.  

```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=6, fig.width=12}

met1 <- rfeKtuneThresh[,c(3,4,5,18)]
met1 <- reshape2::melt(met1, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

m1 <- ggplot(met1, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("55%")

met2 <- rfeKtuneprevThresh[,c(3,4,5,18)]
met2 <- reshape2::melt(met2, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

m2 <- ggplot(met2, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("10%")

grid.arrange(m1,m2, ncol=2)
```


So like I said above, one way is to use all the data we have (balanced data) and then test on *natural* prevalence rates with the 'vaulted' test set and the other way is to implement the prevalence rate within the training data and the final 'vaulted' test set. The goal is to see how much of a difference these two techniques are and how we can move foward with modeling this type of spatial data, i.e. data that is spatial and target class is rare so that we can have suffient models. Please let me know what your thoughts are as this is very subtle. Now if you want to read below it basically goes into some very nuanced stuff. Read on if you like. Below could be a way to visualize the compromise, i.e. train on 50/50 proportion and assess on different prevalence rates.  


```{r echo=FALSE, fig.align="center"}
thresh.examp %>% ggplot(aes(Prevalence, Kappa, color = Model)) + geom_point() + geom_line() + geom_smooth(method = "lm", se = FALSE)+ xlab("Prevalance rate of Minority (stream occurrence)") + ggtitle("Pick the most stationary model?")

```


When splitting the data a critical part becomes how will you test the data, or how much will you leave out for testing? (side bar - there are a lot of nuances in model selection and validation techniques, all have their place and reasons and there is not a perfect 'one-size-fits-all' approach, but leaving a validation set out for a sanity check is common practice regardless of training techniques). The test set should be sampled to be more representitive of the response frequencies [@kuhn2013applied] if an inherent imbalance occurs naturally (these can change and are dynamic in some cases). So, if you're going to validate on a hold-out test set be sure the test set matches the inherent prevalance rate (most of the time it's unknown and cannot infer...). This becomes tricky again since we are dealing with spatial data (eg., climate, geology, landtype). This seems to be very nuanced and pedantic. I am aware.  

However, this still doesn't get at the issue that others @kuhn2013applied and @roberts2017cross have brought to the users attention, i.e. testing model with a representitive prevalance rate.  This may seem too stringent and unrealistic for most circumstances but in our case we have a sufficient amount of data and we can calculate the prevalance rate in different spatial structures. Thus we will use this methodology for testing the final data along side the traditional approach of using the stratified random sample that was produced when splitting, i.e. your test set is partioned to have a $\approx$ 50/50 split between the binary response proportions. Therefore we will move on with calculated the prevalance rates below.  

I decided to use the climatic water deficit (CWD) tertile method similar to @hoylman2019climatic as a way to break up the sample space into water-limiting and energy-limiting geographic zones. The advatages of using CWD as geographic partitioning is that it takes into account numerous biophysical processes, albeit modeled. Numerous papers have shown the interplay between these processes and the advantage it has over traditional geographic partitioning, e.g. elevation, precipitation, vegetation type, etc [@hoylman2018hillslope; @hoylman2019climatic; @lutz2010climatic; @stephenson1998actual]. From these tertiles we can calculate prior distributions of stream prevalence from watersheds that we have copious amounts of data on, which will help us when we test the data on the final model. These distristributions will then get transferred to a global (whole sample space) and three local (tertiles) geogrpahical zones from which the inherent prevalance rate can be used to test the final selected model on; allowing us to compare and contrast statistics (e.g., Kappa, ROC, etc). This will hopefully give us a more realistic model assessment result to be used when comparing with controls (eg., NHD, PROSPER). However, you can only imagine how low these rates would be!  

That being said, just doing a few distributions one can see that even a low deficit zone (energy limiting) has only a 10% prevalance rate. That means that if we left out 3,000 points to test the final model we would then only have 300 points of "stream" to acutally test the model performance metrics on. So, to balance this problem comes an easy solution; just test both rates (e.g., 50/50 and 'natural'). This is easy because the test set is locked in a 'vault' and not touched until the final model assessment from which we can use either method (training proportion vs 'natural' prevalance) on. No harm, no foul. Just keeping your sanity through all this mess of nuances and tedious minutue is a struggle. Hence I will derive a very basic prior using some approximation methods (e.g. division :). 

First we will split the data into tertiles of CWD. We will clip the data to US Forest Service ground because that's where we want to predict and where our data is from.  


```{r, echo=FALSE, message = FALSE, eval=FALSE}

#mask out CWD from landClip
defTert <- mask(deficit, landClip)
```
Calculate the tertile of `deficit` and use these values to reclassify.  
```{r}
#now calculate the tertile and reclassify

quantile(defTert, probs = c(.33,.66))

defTertRC <- reclassify(defTert, c(0,243,1, 243,313,2, 313,Inf,3))
```

Now Plot  

```{r echo=FALSE ,fig.show = "hold", out.width = "40%"}
plot(defTert, main = "Climatic Water Deficit (CWD)")
plot(defTertRC, main = "Tertiles of CWD reclassified")
```
<br>

Now we have a raster broken into tertiles of CWD. However, we want to be able to sort these characteristics by watershed beacause that's the spatial structure our response is most relavant to, i.e. streams are cumulative to basin architexture and HUCs are perfect fit. So, we will use the 12th HUC watershed as a mediator for CWD and then reclassifing per HUC12 by taking the mode of `deficit`. The reason to use HUC12 is that it limits the amount of spatial autocorrelation when compiling zonal summary statistics, i.e. it's more representative of the overarching CWD and not the discrete CWD value.

```{r eval=FALSE}
library(raster)
library(sf)
library(exactextractr)

HUC12 <- readOGR("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/ksank12thclip.shp")

HUC12sf <- st_as_sf(HUC12)
HUC12sf$mean_Def <- exactextractr::exact_extract(deficit, HUC12sf, 'mean')

#or gather other info
HUC12sf[,c("modeDEF", "countDef")] <- exact_extract(deficit, HUC12sf, c('mode', 'count'))


```
<br>
Now plot

```{r, eval = TRUE, echo=FALSE, fig.show = "hold", out.width = "50%"}
plot(HUC12sf["mean_Def"],  key.pos = 4, 
     key.width = lcm(1.3), key.length = 1.0,
     breaks = c(0,243,313,600), main = "Mean Deficit per HUC12 by tertile breaks")
plot(HUC12sf["modeDEF"],  key.pos = 4,  
     key.width = lcm(1.3), key.length = 1.0,
     breaks = c(0,243,313,600), main = "Mode Deficit per HUC12 by tertile breaks")
plot(HUC12sf["countDef"],  key.pos = 4,
     key.width = lcm(1.3), key.length = 1.0,
     breaks = quantile(HUC12sf$countDef), main = " Deficit per HUC12 by quantiles of Count")
```
<br>

I still want to use the mode after looking at the plots as it represents the processes of CWD more clearly than the mean. I think this can be seen in the histograms below. The mean tends to smooth out the HUC12 watersheds when in fact there are more "drier" type landscapes in the valley, i.e. valley may have a few larger streams/rivers carving through but adjacent hillslope processes are most likely drier and this rationale gets smoothed with the mean.  

```{r eval = TRUE, echo=FALSE, fig.show = "hold", out.width = "50%"}
hist(HUC12sf$modeDEF, main = "Histogram of Mode Deficit per HUC12", xlab = "Mode of Deficit per HUC12")
hist(HUC12sf$mean_Def, main = "Histogram of Mean Deficit per HUC12", xlab = "Mean Deficit per HUC12")
```
<br>  
<br>
Now I'll select HUC12's that we have copious amounts of data on where we can then extract a distribution from. Visualize in ArcGIS to get the names.  

<br>
<br>

```{r echo=FALSE, fig.align = "center", out.width = "50%"}
nam <- c("Tobacco River", "Meadow Creek", "Upper Pinkham Creek", "Sutton Creek", "Boulder Creek", "Sullivan Creek")
densityHUCs <- HUC12sf[HUC12sf$Name %in% nam,]
plot(densityHUCs["modeDEF"],key.pos = 4,  
     key.width = lcm(1.3), key.length = 1.0,
     breaks = c(0,243,313,600), main = "Mode Deficit per HUC12")
```
<br>
<br>
From here we can sample from these different HUC12s and come up with a basic and I mean basic idea of stream prevalance. Remember, this is just to help us test the final model during assessment. Very tedious, possibly stringent, and most likely a little off; but, will be more representitive of a *natural* stream prevalance rate. For these type analysis it might not be neccessary (i.e. minimal difference) but others [@kuhn2013applied; @roberts2017cross] have alluded to the importance and thus we will see how it does with our data. It's important to note the differences between detecting certain responses and the wiggle room that's acceptible. For example, cancer and stream detection are extremely different; hence it's probably worth the trouble of finding a prevalance rate for cancer detection than streams but this is up to the goals and objectives of the project in the first. All we are doing is exploring this option to get a glimpse. My personal bias is that it matters and will give a more realistic final model assessment; however, that's my bias so I account for the confirmation bias in the results/discussion section, i.e. take a hard look. So, please take these concepts into consideration when reviewing this section in such that the main goal is hypothesis testing not hypothesis generation. Hopefully this will give a glimse into some of the nuanced debates about testing spatial data on prior distributions. Onward!  

Now we need to rasterize the polygons and points so we can sample.  

```{r, eval=FALSE}
#polygon to raster
deezy <- as_Spatial(densityHUCs)
deezy <- spTransform(deezy, tpiCRS)
densHUC12rast <- rasterize(deezy, deficit, name="values")

#now points to raster

ptsRast <- rasterize(pts, deficit , name="values")

#now select only raster values equal to 1
filter_pts <- ptsRast$copy_TWI_1 %in% 1
```
<br>

After a quick view it was apparant that the point method was not going to work. Thus I pivoted and used an accumulation threshold to be sampled as the global density. The reason for doing this is it's a balance between the results being 0 for a dry area and 5% for wetter areas. I felt this was an appropriate compromise. Therefore, we will only use a *global* prevalance rate for the final model assessment against the 50/50.  

```{r eval=FALSE}
#then extract from the polygons
accumThresh <- reclassify(accum30, c(-Inf, 10000, 1, 10000, Inf, 2))
rHucs <- mask(accumThresh, densHUC12rast)
rHucs <- mask(rHucs, landClip)
writeRaster(rHucs, "D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/rHucs.tif", overwrite = TRUE)
```
```{r, echo=FALSE, fig.align = "center", out.width = "50%"}
hillshade <- raster("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/hillshade.tif")
plot(rHucs, breaks = c(0,.5, 1), col = c("grey", "black"), main = "HUC12 used for sampling");plot(hillshade, add = TRUE, alpha = 0.3, col=grey(1:100/100), legend = FALSE)

```
<br>

From here we can sample from these watersheds to get an idea of the distribution.

```{r}
global.dist <- data.frame(sampleRandom(rHucs, size = 100000))
kableExtra::kable(table(global.dist), caption = "Global Distribution from Random Sample")
```
<br>

So, as I said earlier; tedious, stringent, and most likely trivial. We ended up with 4%. So you can see how low a conservative stream prevalance rate would be. I'm sure you're thinking well duh. But, I had to get it out there. 



### Data Leakage  

**Minimize Data Leakage**  

Data leakage is a term used in data mining where information has been *leaked* and has had a *peak* on the training data [@kaufman2012leakage]. It's usually found when the results are "too good to be true" or unusual variables climb up to the top of variable importance (e.g. coordinates, elevation, ids, time-stamps, etc). Hopefully the investigation we did during predictor selection helped reduce that chance but mistakes can and will be made. Thus using some sort of feature selection technique like @meyer2018improving or spatial cross validataion can help with mitigating variable leakage; however, there are some less obvious ways that data leakage can happen i.e. data splitting.  

When splitting your data for training/tuning and testing, this is a perfect time for data leakage to happen. One common mistake is to downsample/upsample your raw data and then split it into training and test sets after doing this resampling procedure. You just committed the ultimate leakage sin. You might be thinking "this is so harmless" but this allows information from the training data to be passed to the test data via the resampling procedure, which can possibly allow the model to overfit by reducing the actual *population* thus possibly trimming the data (very subtle). Another is if you are imputing or transforming any data pre-data-splitting, then the test data has *seen* the training data and will lead to over-fitting in the model, again very subtle. Finally, when deriving cutoff thresholds it might be tempting to use the same data you used for training or some sort of post-hoc derivation, again this is leakage [@kuhn2013applied]. Therefore, to avoid these pitfalls we looked at data that could be surrogate variables (cause and effect section), we will transform data after data-splitting and during CV (i.e. test sets in CV can't see transformed data, very subtle) or downsample during CV if needed and finally derive thresholds off an independent evaluation set or during the validation process using `recipes`. Since we have a lot of data we have the adantage of splitting the data into multiple sets that will help avoid these issues. However some times this isn't the case so exploring these pitfalls should always be good practice to help us avoid the wrath of data leakage in the future.

### Structures 

**Access Dependance Structures**  

Accessing dependance structures is a step recommended by @roberts2017cross and should be considered common practice with any spatial prediction type analysis. The objective is to analyze your covariates (e.g. rasters) so that you can get an idea of the associated spatial autocorrelation levels (e.g. ranges from variogram). The method that is used in concurrence with @roberts2017cross via @valavi2018blockcv is a variogram type approach (isotopic). The main reason to use this procedure is that if you don't structure your spatial data systematically (i.e. in this case by variograms) then you are likely going to underestimate or overestimate (depends on metrics) your error in your evaluation, again leading to over optimistic results and over-fitting in the models [@roberts2017cross; @meyer2018improving; @meyer2019importance].  

@roberts2017cross and @valavi2018blockcv suggest using a blocking system with different techniques (e.g. systematically, checkerboard, or by rows or columns) and then partitioning the sample space based on the set of variogram ranges (median distance selected) [@valavi2018blockcv]. This is a balanced approach to demarcating the dependance structures; however, there are covariates remaining with high spatial autocorrelation (i.e. median is the threshold). Again, no free lunch. However, there are ways to find a balance; we will search for this balance by testing four different structures, all with there inherent spatial autocorrelation ranges, e.g. 12th HUC, blockCV median, 14th HUC and K-means. The goal is to go big and small, i.e. minimal autocorrelation to a lot.  

Below is how we got the block cv size and how we are adding HUCs to the structure cv using @valavi2018blockcv. This is a really sweet package with great tools and interface options for spatial modelling! Here is a link to the vignette [blockCV](http://htmlpreview.github.io/?https://github.com/rvalavi/blockCV/blob/master/vignettes/BlockCV_for_SDM.html).   


```{r, eval=FALSE}
remotes::install_github("rvalavi/blockCV", dependencies = TRUE)
library(blockCV)
library(sf)
library(raster)
```

First we want to bring in our point data and our covariates. 
```{r, eval=FALSE}
library(sf)
ptsSF <- arc.open("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/points_spaced30m.shp")  #points to the folder/feature
ptsSF <- arc.select(ptsSF, c("copy_TWI_1")) #selects the data in the folder

ptsSF <- arc.data2sf(ptsSF)
ptsSF <- st_as_sf(data, coords = c("coords.x1","coords.x2"))
pts
st_crs(ptsSF) <- "+proj=aea +lat_1=46 +lat_2=48 +lat_0=44 +lon_0=-109.5 +x_0=600000 +y_0=0 +datum=NAD83 +units=m +no_defs +ellps=GRS80 +towgs84=0,0,0"
nrow(data)
topo_new <- dropLayer(topo_opt_rad30, c("ndvias30agg", "cpg30Deficit")) #covariate stack
topo_new
```
<br>
Plot to make sure everything is lined up. Looks good.  

```{r, echo=FALSE, fig.align="center", out.width="50%", message = FALSE}
#plot to double check
plot(topo_opt_rad30[[1]], legend = FALSE); plot(hillshade, alpha = 1/10, legend = FALSE, col = grey(1:100/100), add = TRUE);
plot(ptsSF[which(ptsSF$stream==0), ], pch = 16, col="red", add=TRUE); 
plot(ptsSF[which(ptsSF$stream==1), ], pch = 1, col="blue", add=TRUE, cex = 0.1);
legend(x=140000, y=570000, legend=c("Absence","Presence"), col=c(2, 4), pch=c(16,1), bty="n")

```

<br>

Now we can figure out our 'effictive range of spatial autocorrelation' by using the function `spatialAutoRange` in `blockCV`. This is a cool function because it takes the covariates, calculates the range of autocorrelation (from $n$ sampled points), and then produces two plots showing the range of the covariates and the recommended block size for validation.  

```{r eval=FALSE}

sac <- blockCV::spatialAutoRange(rasterLayer = topo_new,
                                 sampleNumber = 5000,
                                 doParallel = TRUE,
                                 showPlots = TRUE)

```

```{r, echo=FALSE, fig.align="center"}

plot(sac)

```
Plots of variograms: lowest (TPI), median (UAA), highest (CWD).  
<br>
```{r eval = TRUE, echo=FALSE, fig.show = "hold", out.width = "50%"}
library(automap)
par(mfrow = c(3,1))
plot(sac$variograms[[2]])
plot(sac$variograms[[3]])
plot(sac$variograms[[10]])



```
<br>

So as you can see there are some variables that are very spatially autocorrelated. The `BlockCV` package recommends the median range as a structure dimension and then disects the sample space into dimensions, e.g. blocks. From here you add the points and `BlockCV` will provide a sampling scheme for cross validation. Pretty sweet! Real quick, here's the summary from the variograms giving the range with each covariate.  


```{r echo=FALSE}
library(kableExtra)
kable(sac_summary, caption = "Table of Range and Covariates") %>%
  kable_styling(bootstrap_options = c("striped", "hover","condensed", "responsive"),full_width = F, position = "center")
```
<br>

Now we can systematically seperate the blocks out with a k=10 for a 10-fold CV and a range from the derived median.

```{r eval=FALSE}

sbMed <- spatialBlock(speciesData = ptsSF, # presence-background data
                    species = "stream",
                    rasterLayer = topo_new,
                    k = 10,
                      theRange = 5694,
                    selection = "systematic",
                    biomod2Format = TRUE)


```
```{r eval = TRUE, echo=FALSE, fig.align="center", out.width = "50%"}
plot(sbMed$plots)
```
<br>
<br>

Now we can partition the data by Hydrological Unit Codes (12th and 14th HUCs) into 'blocks' based on a systematic approach. Same thing: 10 folds, selection = "systematic". 


```{r eval=FALSE}

HUC14SP <- arc.open("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/ksank14thclip.shp")  #points to the folder/feature
HUC14SP <- arc.select(HUC14SP, c("FID")) #selects the data in the folder

HUC14SP <- arc.data2sp(HUC14SP)

HUC14SP <- spTransform(HUC14SP,CRSobj = crs(topo_opt_rad30))

sb14 <- spatialBlock(speciesData = ptsSF, 
                    species = "stream", 
                    rasterLayer = topo_new, 
                    blocks = HUC14SP,
                    k = 10,
                    selection = "systematic")


```
```{r echo=FALSE}
sb14$plots + ggtitle("14th HUC")
```


```{r eval=FALSE}
HUC12SP <- arc.open("D:/Rcodes/Water_Prediction/Hird_Water_Prediction/waterD/waterPred/workingThoughProject/ksank12thclip.shp")  #points to the folder/feature
HUC12SP <- arc.select(HUC12SP, c("FID")) #selects the data in the folder

HUC12SP <- arc.data2sp(HUC12SP)

HUC12SP <- spTransform(HUC12SP,CRSobj = crs(topo_opt_rad30))

sb12 <- spatialBlock(speciesData = ptsSF, 
                    species = "stream", 
                    rasterLayer = topo_new, 
                    blocks = HUC12SP,
                    k = 10,
                    selection = "systematic")
```

```{r echo=FALSE}
sb12$plots + ggtitle("12th HUC")
```
<br>

Finally, we can do a `kmeans` with a cluster of 20. This will be our small structure attempt.

```{r eval = FALSE}
#this is the Leave-One-Out-CV (LOOCV) method (using a cluster of 20)
#need coordinates.
#remember to use the coordinates, that's why we've kept them!

Mycluster <- kmeans(data[,c(23,24)], (nrow(data)/20)) 

# add the new variable back to your dataframe here
data$spatial_cluster =Mycluster$cluster
```

Now you can see the clusters.  

```{r, echo=FALSE, fig.align="center"}
#just to visualize!
plot(hillshade, col = grey(1:100/100), legend = FALSE);plot(ptsSF["spatial_cluster"], add = TRUE) 
```

<br>

Now bring in the blocks we created (`sb12`,`sb14`,`sb2`) and bind with our data frame `data` so that we can index these during CV.
```{r eval=FALSE}

data <- cbind(data, sbMed = sbMed$foldID, sb14 = sb14$foldID, sb12 = sb12$foldID)
data[1:20,c(23:26)]
data <- data[,-c(28,29,30)]

```



<br>





## Evaluation Techniques  

**Evaluation Techniques**  

Building a model can seem pretty straight forward and easy, almost like a plug and chug; but, this will most likely lead to unreliable results and dissappointed end-users [@kuhn2013applied]. We want to correct for these potential oversights as discussed in earlier sections and equally not take so much time while doing so (i.e. model efficiency). Therefore, balancing validation/selection techniques with time will be the goal of this analysis so that future workflows can be performed in a modest amount of time while also achieving optimal model performance within a spatial context. Traditional ways of balancing these issues can be through different validation techniques (e.g., k-Fold Cross-Validation, bootstrapping (out-of-bag error), validation set approach, Leave-One-Out Cross-Validation (LOOCV), etc) and feature selection techniques (e.g., forward first, recursive, or natural feature extraction (lasso)) [@friedman2001elements;@james2013introduction;@kuhn2013applied]; however, within a spatial context these techniques can run into problems as found in [@meyer2018improving;@meyer2019importance; @roberts2017cross]. Therefore we must account for these problems and use other techniques to lessen the effect of overfitting within a spatial context. 

As I've already alluded to in previous sections, we will use a feature selection model from @meyer2018improving which is a forward feature selection (ffs) type technique within then `CAST` package and compare with a recursive feature elimination (rfe) technique from @kuhn2013applied. This is very similar to the approach that @meyer2018improving;@meyer2019importance has done. In addition, we will also use four different target-oriented cross-validation approaches to find a dependance structure balance between autocorrelation and evaluation (eg., HUC12, BlockCV, HUC14 and K-means). The importance of feature selection and evaluation (eg., cross-validation, out-of-bag error, etc) within a spatial context has been expressed in numerous papers [@bahn2013testing;@ives2006statistics;@meyer2018improving;@meyer2019importance; @roberts2017cross], and the the reason why we're putting so much emphasis on this topic is that we want be confident about the performance statistics before the model is going into production, i.e. so that the end-user will not be mislead. How these frameworks play into management is a matter of the objectives and goals; therefore, in the spatial domain where highly flexible models (i.e. ensemble type models) are becoming the norm cation should be met. 


**Feature Selection**  

I highlighted some advantages of using a feature selection in previous sections, but there can also be some disadvantages as well. One of the most recognized problems with feed forward type selection (e.g. stepwise, forward first, etc) is that the model algorithm puts a threshold on how far the model will go before the *selections* stop , which leaves the possibility of important variables not being selected [see @friedman2001elements;@james2013introduction;@kuhn2013applied;@meyer2018improving]. To possibly account for this affect, there are ways or options within the `CAST` package to model all possible best subsets (bss), but will take ages! Discusssing this with one of the authors of [@meyer2018improving; @meyer2019importance], she said "I compared bss with fss for some examples and found that the ffs is doing a good job and rarely misses important variables."  

Therefore, ffs seems get it right a majority of the time. Is this enough? I think exploring the theory behind ffs within a spatial context might help explain this question or concern. To solve its problems of occasionally not picking an 'important' predictor (that is ffs) it emphasizes  combinations of variables that reduce variance in such reduce overfitting. Again, there are trade-offs and it comes down to how much time and energy you want to put into capturing the covariate(s) (e.g. bss will do this) that will possibly increase model performance. For example, as discussed in earlier sections we want to avoid surrogate or geolocated-type variables because of the overfitting that occurs when these variables are in a model [@meyer2018improving; @meyer2019importance]; therefore, by reducing the overfitting up-front (ffs) compared to the tradeoff of possibly missing an important variable during selection (i.e. interpretable or optimal, goals and objectives matter...) outweighs the cost of having an over-optimistic model for the end-user.  

The compliment of ffs is rfe and @meyer2019importance has shown that within a spatial context the rfe approach can lead to overfitting. This overfitting can happen due to the algorithm holding on to important variables that may have significant variance that goes unoticed [@kuhn2013applied; @meyer2019importance]. We want to see how these two feature selection approaches deal with the stream occurrence data. They most likely will be similar due to the stringent pre-processing we did and @meyer2019importance has noted that rfe can achieve similar results with ffs; it just depends on the data being used. That is to say if you pre-process your data with 'cause and effect' in mind, then the differences may be obsolete. We will test this hypothesis.   


**Tuning/Evaluation**  

Tuning and evaluation go hand-in-hand. We will set aside a tuning/evaluation set that hasn't seen the feature selection sets (e.g. ffs, RFE). The reason is due to data leakage and the overfitting caused by ovlapping, i.e. training with testing data. During this process we will tune the parameters of the models so that we can achieve otimal performance. There are different techniques to do this like a 'search' function for the most optimal parameters, which randomly searches different parameters in randomly uniform sample. I will use this for the gradient boosting machine (GBM) using methods from @kuhn2013applied but not for random forest, due to random forest having minimal parameters. This will hopefully dismiss irrelevant parameters and focus on useful ones in a time-saving manner, i.e. less models will be fitted compared to a expansive grid approach. 

Along with the tuning process we will also be doing cutoff threshold derivations so we can descriptively look into the interplay between specificity/sensitivity which some refer to recall/precision. This is a confusing topic that involves different confusing confusion matrix schemes online. In short, `caret` uses the @altman1994diagnostic method which is normally what you'll find in literature; however others 'online' will use an alternative which has the table flip-flopped, e.g. actual and predicted get switched but the terms don't! Be aware of this and when reading this workflow refer to this link [here](https://rdrr.io/cran/caret/man/sensitivity.html) or [here](https://en.wikipedia.org/wiki/Confusion_matrix) for more on confusion matrix terminology and the specific method we'll be using.  

Within classification models and imbalanced data or rare data the goal is to optimize the splitting thresholds (e.g. standard cutoffs are at 50%) so that performance statistics can be reported to reflect the 'on the ground' transitions from 'no stream' to 'stream' appropriately. Thus one approach is to select for the optimal model by the Receiver operating characteristic (ROC). When compared to soley selecting for accuracy this can lead to a more balanced model performance between sensitivity/specificity (i.e. cutoff threshold of 50%); however, it is not always the optimal goal for the project like reducing false positives. To achieve optimal ROC there are methods like Youdens J or distance between Sens/Spec (Dist) that can increase ROC while balancing Sens/Spec. This is a pretty balanced approach because it balances the false positives and negatives.  

By deriving a cutoff threshold that will balance the two type I/II errors, we can produce a more accurate model. There are tradeoffs of course, again no free lunch. The tradeoff come with giving or taking; from either false positive or negative. In our case, this will most likely reduce the false positives but increase the false negatives, which is what we want. So, deriving cutoff thresholds can help see these tradeoffs visually and can help find an optimal cutoff threshold to balance the two errors; however, it can also go against your objectives so be careful. We will just look at the possibilites but will ultimately go with the initial ROC/AUC value from the model,i.e. similar to @hird2017google. We could however pick a model that has the best specificity (e.g. less false positives) instead of using the ROC/AUC. This approach can help us achieve our objectives but the tradeoff is possibly having a model that is less balanced, which may be an alternative. To avoid any data leakage, typically you would want to do your tuning and evaluating (ROC/AUC) on a seperate hold out sets, e.g. tuning on one and evaluating on another. However, there are ways that you can combined these techniques together so that you don't have to split the data up, which is what we will do.  

The reason to do it within the tuning/evaluation set is 1). we don't have to create another partition of data (e.g. a set), 2). From this we will be able to use more data in the final test set and also in the tuning/evalution set, 3). It keeps everything tidy. That being said, I also think by doing it all together and keeping the results tidy it will be easier for performance exploration and visualization. Within this process you can do any prep work that you would want to do without violating data leakage scenarios, e.g. transform, PCA, down-upsample, etc. We will do this with the PCA variables in our data set, e.g. B2, B3, B4, B8 and will bring some variables along for the ride to use later, e.g. *post hoc* visualization.  


# Modeling   

## Model Selection  

**Model Splitting**  

 Above in the data splitting section I  proposed that we wanted to split the data into three partitions with the test data being the final model performance measure (sanity check). To do this we will split the data into two chunks and then split into test proportions once our final model is selected. This will hopefully give us a *natural* statistic into the final model performance. 

```{r, eval = FALSE}
library(caret)
library(CAST)



set.seed(1234)
#split data into training and test data. 75% for training is the goal. 
datindex <- createDataPartition(data$stream, list = FALSE, p=0.75)

train <- data[datindex,]

test <- data[-datindex,]

#then split the training data into two so we have 'overall' 25% for feature selection and 'overall' 50% for training.
set.seed(1234)
trainindex <- createDataPartition(train$stream, list = FALSE, p = .66)

traintune <- train[trainindex,]

trainsel <- train[-trainindex,]

trainprevtune <- train[trainindex,]

trainprevsel <- train[-trainindex,]

#now for the tuning training set using prevalence
trainprevtuneSample1 <- trainprevtune %>% filter(stream == "1") 

set.seed(1234)
 trainprevtuneSample1 <- trainprevtuneSample1[sample(nrow(trainprevtuneSample1),274),]
 
trainprevtuneSample0 <- trainprevtune %>% filter(stream == "0") 

  trainprevtune <- data.frame(rbind(trainprevtuneSample0,trainprevtuneSample1))
  
  summary(trainprevtune$stream)

#now for the selection training set using prevalence
trainprevselSample1 <- trainprevsel %>% filter(stream == "1") 

set.seed(1234)
 trainprevselSample1 <- trainprevselSample1[sample(nrow(trainprevselSample1),141),]
 
trainprevselSample0 <- trainprevsel %>% filter(stream == "0") 

  trainprevsel <- data.frame(rbind(trainprevselSample0,trainprevselSample1))
  
  summary(trainprevsel$stream)


```

As we've discussed earlier, another important step in evaluating spatial data is accounting for spatial autocorrelation. We can do this by doing spatial cross validation where folds are created at random; however, when specified (i.e., HUCs, block, cluster) these folds are systematically seperated by k-folds [@roberts2017cross; @valavi2018blockcv]. This will help correct for any spatial autocorrelation in the validation process, which will lead to more realistic results [@meyer2018improving; @meyer2019importance]. The goal is to see how model performances react with different dependance structures. Hence finding a balance between 'too large' and 'too small.' Below we will use the `CAST` package to index these dependance structures. 

```{r, eval=FALSE}
library(CAST)

#this is the index for the HUC12 method.
set.seed(1234)
indices12sel <- CreateSpacetimeFolds(trainsel, spacevar = "sb12", k = 10)

set.seed(1234)
indices12tune <- CreateSpacetimeFolds(traintune,spacevar = "sb12", k = 10) 

set.seed(1234)
indices12selprev <- CreateSpacetimeFolds(trainprevsel, spacevar = "sb12", k = 10)

set.seed(1234)
indices12tuneprev <- CreateSpacetimeFolds(trainprevtune,spacevar = "sb12", k = 10) 

#now for the blockCV method

set.seed(1234)
indicesMedsel <- CreateSpacetimeFolds(trainsel, spacevar = "sbMed", k = 10)

set.seed(1234)
indicesMedtune <- CreateSpacetimeFolds(traintune,spacevar = "sbMed", k = 10) 

set.seed(1234)
indicesMedselprev <- CreateSpacetimeFolds(trainprevsel, spacevar = "sbMed", k = 10)

set.seed(1234)
indicesMedtuneprev <- CreateSpacetimeFolds(trainprevtune,spacevar = "sbMed", k = 10) 

#this is the index for the HUC14 method

set.seed(1234)
indices14sel <- CreateSpacetimeFolds(trainsel, spacevar = "sb14", k = 10)

set.seed(1234)
indices14tune <- CreateSpacetimeFolds(traintune,spacevar = "sb14", k = 10) 

set.seed(1234)
indices14selprev <- CreateSpacetimeFolds(trainprevsel, spacevar = "sbMed", k = 10)

set.seed(1234)
indices14tuneprev <- CreateSpacetimeFolds(trainprevtune,spacevar = "sbMed", k = 10) 

#now for the cluster method

set.seed(1234)
indicesKsel <- CreateSpacetimeFolds(trainsel, spacevar = "spatial_cluster", k = 10)

set.seed(1234)
indicesKtune <- CreateSpacetimeFolds(traintune,spacevar = "spatial_cluster", k = 10) 

set.seed(1234)
indicesKselprev <- CreateSpacetimeFolds(trainprevsel, spacevar = "sbMed", k = 10)

set.seed(1234)
indicesKtuneprev <- CreateSpacetimeFolds(trainprevtune,spacevar = "sbMed", k = 10) 
```

Now that we have our training data let's start building the models. We are going to do two feature selection models (e.g. rfe and ffs) per spatial CV for two different model types. So in this section there will be 8 different model results. First we need to create a `recipe` to handle the pca extraction during the feature selection.

```{r eval=FALSE}
library(recipes)
library(tidyselect)

#for balanced data
rec_sel <- 
  recipe(stream ~ ., data = trainsel) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9)

rec_tune <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9)

#for prevalence data
rec_selprev <- 
  recipe(stream ~ ., data = trainprevsel) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9)

rec_tuneprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9)

```

#### RFE Random Forest

Now we can start with the rfe feature selection. This feature selection technique [@kuhn2013applied] uses a recursive algorithm which models independent of different variable lengths and then selects the variables that leads to the lowest metric (e.g. Kappa, accuracy, etc). We will use the Kappa statistic as the feature selection criteria. Later in the tuning and the evaluation process we will use Specificity as a metric to evaluate the overall performance. This workflow is very similar to @hird2017google except we will use a feature selection step before tuning/evaluation.  

We will use the default tuning grids for feature seletion below for both Random Forest and GBM (ffs and rfe). The reason has to do with time and it most likely won't get you anything; however, if someone wants to try go right ahead. I'm going to use the tuning/validation set to beef up the models! This is more variable selection, but like I said you may want to add tuning and tailor how you want.



*RFE 12th HUC Random Forest*
```{r, eval = FALSE}

library(plyr)
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrl12 <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all",
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices12sel$index
)
rfeCtrl12prev <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all",
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices12selprev$index
)

#balanced model
set.seed(1234)
rfe12 <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrl12,
  sizes = c(2:18))

#now for prevalence model
set.seed(1234)
rfe12prev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrl12prev,
  sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```
```{r, echo=FALSE}
#now plot
kableExtra::kable(varImp(rfe12))
rfe12$results$model <- "RFE RF12"
rfe12$variables$model <- "RFE RF12"
kableExtra::kable(varImp(rfe12prev))
rfe12prev$results$model <- "RFEprev RF12"
rfe12prev$variables$model <- "RFEprev RF12"
```
```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
c1 <- ggplot(rfe12) + ggtitle("Balanced")
c2 <- xyplot(rfe12, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfe12, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")
c4 <- ggplot(rfe12prev) + ggtitle("Prevalence")
c5 <- xyplot(rfe12prev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfe12prev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")

grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```

*RFE blockCV Random Forest*  

```{r, eval = FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrlMed <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesMedsel$index
)

rfeCtrlMedprev <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesMedselprev$index
)
#balanced data
set.seed(1234)
rfeMed <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrlMed,
  sizes = c(2:18))

#prevalence data
set.seed(1234)
rfeMedprev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrlMedprev,
  sizes = c(2:18))
#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```
```{r, echo=FALSE}
#now plot
kableExtra::kable(varImp(rfeMed))
rfeMed$results$model <- "RFE RFMed"
rfeMed$variables$model <- "RFE RFMed"
kableExtra::kable(varImp(rfeMedprev))
rfeMedprev$results$model <- "RFEprev RFMed"
rfeMedprev$variables$model <- "RFEprev RFMed"
```
```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
c1 <- ggplot(rfeMed) + ggtitle("Balanced")
c2 <- xyplot(rfeMed, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfeMed, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")

c4 <- ggplot(rfeMedprev) + ggtitle("Prevalence")
c5 <- xyplot(rfeMedprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfeMedprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")
grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```

*RFE 14th HUC Random Forest*
```{r, eval = FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrl14 <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices14sel$index
)

rfeCtrl14prev <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices14selprev$index
)
#balanced data
set.seed(1234)
rfe14 <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrl14,
  sizes = c(2:18))

#prevalence data
set.seed(1234)
rfe14prev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrl14prev,
  sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```
```{r, echo=FALSE}
kableExtra::kable(varImp(rfe14))
rfe14$results$model <- "RFE RF14"
rfe14$variables$model <- "RFE RF14"
kableExtra::kable(varImp(rfe14prev))
rfe14prev$results$model <- "RFEprev RF14"
rfe14prev$variables$model <- "RFEprev RF14"
```
```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
c1 <- ggplot(rfe14) +ggtitle("Balanced")
c2 <- xyplot(rfe14, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfe14, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")

c4 <- ggplot(rfeMedprev) + ggtitle("Prevalence")
c5 <- xyplot(rfeMedprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfeMedprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")
grid.arrange(c1,c4,c2,c5,c3, c6, ncol = 2)
```

*RFE Cluster Random Forest*

```{r, eval = FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrlK <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesKsel$index
)

rfeCtrlKprev <- rfeControl(
  functions = rfFuncs,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesKselprev$index
)

#balanced data
set.seed(1234)
rfeK <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrlK,
  sizes = c(2:18))

#prevalence data
set.seed(1234)
rfeKprev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrlKprev,
  sizes = c(2:18))


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```
```{r echo=FALSE}
kableExtra::kable(varImp(rfeK))
rfeK$results$model <- "RFE RFK"
rfeK$variables$model <- "RFE RFK"
kableExtra::kable(varImp(rfeKprev))
rfeKprev$results$model <- "RFEprev RFK"
rfeKprev$variables$model <- "RFEprev RFK"
```
```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
c1 <- ggplot(rfeK) + ggtitle("Balanced")
c2 <- xyplot(rfeK, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfeK, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")
c4 <- ggplot(rfeMedprev) + ggtitle("Prevalence")
c5 <- xyplot(rfeMedprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfeMedprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")

grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```


```{r eval=FALSE, echo=FALSE}
RFEsum <- rbind(rfe12$results, rfeMed$results, rfe14$results, rfeK$results,
                rfe12prev$results, rfeMedprev$results, rfe14prev$results, rfeKprev$results)
```

```{r, echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
library(gridExtra)
a1 <- RFEsum %>% ggplot(aes(Variables, Kappa, color = model)) + geom_point() + geom_line() + ggtitle("Kappa per Variable per model")
a2 <- RFEsum %>% ggplot(aes(Variables, Accuracy, color = model)) + geom_point() + geom_line()+ ggtitle("Accuracy per Variable per model")
a3 <- RFEsum %>% ggplot(aes(Variables, KappaSD, color = model)) + geom_point() + geom_line()+ ggtitle("KappaSD per Variable per model")
a4 <- RFEsum %>% ggplot(aes(Variables, AccuracySD, color = model)) + geom_point() + geom_line()+ ggtitle("AccuracySD per Variable per model")

grid.arrange(a1,a2,a3,a4, ncol = 2)
```


As we expected, when you go from large hold outs to small you will see a trend in performance. 

#### RFE GBM

Now we can move on to the GBM rfe. First we need to modify and bring in `gbm` properties for rfe so that it can recognize the gbm operations, similar to rfFuncs. This is due to rfe not having a predefined function for `gbm`.  


```{r eval = FALSE}

## this just brings in a basic list from which we can then modify
gbmRFE <-  list(summary = defaultSummary,
               fit = function(x, y, first, last, ...){
                 library(randomForest)
                 randomForest(x, y, importance = first, ...)
               },
               pred = function(object, x)  predict(object, x),
               rank = function(object, x, y) {
                 vimp <- varImp(object)
                 vimp <- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
                 vimp$var <- rownames(vimp)                  
                 vimp
               },
               selectSize = pickSizeBest,
               selectVar = pickVars)

#summary function is how are we storing the stats per resample
gbmRFE$summary <- function (data, lev = NULL, model = NULL) 
{
  if (is.character(data$obs)) 
    data$obs <- factor(data$obs, levels = lev)
  postResample(data[, "pred"], data[, "obs"])
}


#now bring in the gbm function and use method = "gbm"
gbmRFE$fit <- function (x, y, first, last, ...) {
  library(gbm)
  caret::train(x, y, method = "gbm")}

#this is the predict function from the "gbm" package
gbmRFE$pred <- function (object, x) 
{
  tmp <- predict(object, x)
  if (object$modelType == "Classification" & object$control$classProbs) {
    out <- cbind(data.frame(pred = tmp), as.data.frame(predict(object, 
                                                               x, type = "prob")))
  }
  else out <- tmp
  out
}

#this is the rank function from the "gbm" package
gbmRFE$rank <- function(object, x, y){
  vimp <- varImp(object, scale = TRUE)$importance
  if (!is.data.frame(vimp)) 
    vimp <- as.data.frame(vimp)
  if (object$modelType == "Regression") {
    vimp <- vimp[order(vimp[, 1], decreasing = TRUE), , drop = FALSE]
  }
  else {
    if (all(levels(y) %in% colnames(vimp)) & !("Overall" %in% 
                                               colnames(vimp))) {
      avImp <- apply(vimp[, levels(y), drop = TRUE], 1, 
                     mean)
      vimp$Overall <- avImp
    }
  }
  vimp <- vimp[order(vimp$Overall, decreasing = TRUE), , drop = FALSE]
  vimp$var <- rownames(vimp)
  vimp
}

#selectSize and selectVar will stay the same

```

Now we can start modeling. It should be noted that `gbm` does a  internal feature selection so this might not be necessary; however, i'm investigating some more and still think that it is applicable for size of variables and trends.  

*RFE 12th HUC GBM*  

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrl12g <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices12sel$index
)

rfeCtrl12gprev <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices12selprev$index
)

#Balanced 
set.seed(1234)
rfe12g <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrl12g,
  sizes = c(2:18))

#Prevalence
set.seed(1234)
rfe12gprev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrl12gprev,
  sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```


```{r echo=FALSE}
#now plot
kableExtra::kable(varImp(rfe12g))
rfe12g$results$model <- "RFE GBM12"
rfe12g$variables$model <- "RFE GBM12"
kableExtra::kable(varImp(rfe12gprev))
rfe12gprev$results$model <- "RFEprev GBM12"
rfe12gprev$variables$model <- "RFEprev GBM12"
```

```{r , echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
c1 <- ggplot(rfe12g) + ggtitle("Balanced")
c2 <- xyplot(rfe12g, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfe12g, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")
c4 <- ggplot(rfe12gprev) + ggtitle("Prevalence")
c5 <- xyplot(rfe12gprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfe12gprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")

grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```

*RFE blockCV GBM*  

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrlMedg <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesMedsel$index
)

rfeCtrlMedgprev <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesMedselprev$index
)

#balanced
set.seed(1234)
rfeMedg <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrlMedg,
  sizes = c(2:18))

#prevalence
set.seed(1234)
rfeMedgprev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrlMedgprev,
  sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()
```

```{r echo=FALSE}
#now plot
kableExtra::kable(varImp(rfeMedg))
rfeMedg$results$model <- "RFE GBM Med"
rfeMedg$variables$model <- "RFE GBM Med"
kableExtra::kable(varImp(rfeMedgprev))
rfeMedgprev$results$model <- "RFEprev GBM Med"
rfeMedgprev$variables$model <- "RFEprev GBM Med"
```

```{r , echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}

c1 <- ggplot(rfeMedg) + ggtitle("Balanced")
c2 <- xyplot(rfeMedg, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfeMedg, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")

c4 <- ggplot(rfeMedgprev) + ggtitle("Prevalence")
c5 <- xyplot(rfeMedgprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfeMedgprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")

grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```


*RFE 14th HUC GBM*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrl14g <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices14sel$index
)

rfeCtrl14gprev <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indices14selprev$index
)

#balanced
set.seed(1234)
rfe14g <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrl14g,
  sizes = c(2:18))

#prevalence
set.seed(1234)
rfe14gprev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrl14gprev,
  sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```

```{r echo=FALSE}
#now plot
kableExtra::kable(varImp(rfe14g))
rfe14g$results$model <- "RFE GBM 14"
rfe14g$variables$model <- "RFE GBM 14"
kableExtra::kable(varImp(rfe14gprev))
rfe14gprev$results$model <- "RFEprev GBM 14"
rfe14gprev$variables$model <- "RFEprev GBM 14"
```

```{r , echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}

c1 <- ggplot(rfe14g) + ggtitle("Balanced")
c2 <- xyplot(rfe14g, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfe14g, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")

c4 <- ggplot(rfe14gprev) + ggtitle("Prevalence")
c5 <- xyplot(rfe14gprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfe14gprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")
grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```

*RFE Kmeans GBM*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#add to controls
rfeCtrlKg <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesKsel$index
)

rfeCtrlKgprev <- rfeControl(
  functions = gbmRFE,
  method = "repeatedcv",
  repeats = 5,
  returnResamp = "all", 
  verbose = FALSE, 
  allowParallel = TRUE,
  index = indicesKselprev$index
)

#balanced
set.seed(1234)
rfeKg <- rfe(rec_sel,
  data = trainsel, metric = "Kappa",
  rfeControl = rfeCtrlKg,
  sizes = c(2:18))

#prevalence
set.seed(1234)
rfeKgprev <- rfe(rec_selprev,
  data = trainprevsel, metric = "Kappa",
  rfeControl = rfeCtrlKgprev,
  sizes = c(2:18))

#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```

```{r echo=FALSE}
#now plot
kableExtra::kable(varImp(rfeKg))
rfeKg$results$model <- "RFE GBM K"
rfeKg$variables$model <- "RFE GBM K"
kableExtra::kable(varImp(rfeKgprev))
rfeKgprev$results$model <- "RFEprev GBM K"
rfeKgprev$variables$model <- "RFEprev GBM K"
```

```{r , echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}

c1 <- ggplot(rfeKg) + ggtitle("Balanced")
c2 <- xyplot(rfeKg, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Balanced")
c3 <- densityplot(rfeKg, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Balanced", 
                     pch = "|")

c4 <- ggplot(rfeKgprev) + ggtitle("Prevalence")
c5 <- xyplot(rfeKgprev, 
                type = c("g", "p", "smooth"), 
                ylab = "Kappa CV Estimates Prevalence")
c6 <- densityplot(rfeKgprev, 
                     subset = Variables %in% c(6:11), 
                     adjust = 1.25, 
                     as.table = TRUE, 
                     xlab = "Kappa CV Estimates Prevalence", 
                     pch = "|")

grid.arrange(c1,c4,c2,c5,c3, c6, ncol=2)
```



```{r eval=FALSE, echo=FALSE}
RFEsumg <- rbind(rfe12g$results, rfeMedg$results, rfe14g$results, rfeKg$results,
                 rfe12gprev$results, rfeMedgprev$results, rfe14gprev$results, rfeKgprev$results)
```

```{r, echo=FALSE, message=FALSE, fig.align='center', fig.height=10, fig.width=12}
library(gridExtra)
a1 <- RFEsumg %>% ggplot(aes(Variables, Kappa, color = model)) + geom_point() + geom_line() + ggtitle("Kappa per Variable per model")
a2 <- RFEsumg %>% ggplot(aes(Variables, Accuracy, color = model)) + geom_point() + geom_line()+ ggtitle("Accuracy per Variable per model")
a3 <- RFEsumg %>% ggplot(aes(Variables, KappaSD, color = model)) + geom_point() + geom_line()+ ggtitle("KappaSD per Variable per model")
a4 <- RFEsumg %>% ggplot(aes(Variables, AccuracySD, color = model)) + geom_point() + geom_line()+ ggtitle("AccuracySD per Variable per model")

grid.arrange(a1,a2,a3,a4, ncol = 2)
```

As we expected, when you go from large hold outs to small you will see a trend in performance. 


#### FFS Random Forest

So with the FFS we will just use the default parameters for both Random Forest and GBM. This shouldn't be too big of concern as I mentioned earlier. Kappa will still be the performance metric for selecting. First we need to prep the PCA variables before we do ffs. 

```{r, eval=FALSE}
#NEED to prep the variables for PCA

set.seed(1234)
library(recipes)

rec_prepSel <- prep(rec_sel,training = trainsel, retain = TRUE)
rec_prepSel$var_info %>% filter(rec_prepSel$var_info$role == "predictor") %>% .$variable
set.seed(1234)
rec_selJuiced <- juice(rec_prepSel,composition = "data.frame")

predictorsRF <- rec_selJuiced[,c("twi30agg","tpi30agg","accum30","vv30agg", "vvsd30agg","npol30agg","ndvi30yrRS", "ndwias30agg","nppmmid30agg","deficitRS",   "hdi30RS","cad30RS","decid30RS","cpg30precip", "pca_B1")]

responseRF <- rec_selJuiced$stream

#now do for prevalence
rec_prepSelprev <- prep(rec_selprev,training = trainprevsel, retain = TRUE)

set.seed(1234)
rec_selJuicedprev <- juice(rec_prepSelprev,composition = "data.frame")

predictorsRFprev <- rec_selJuicedprev[,c("twi30agg","tpi30agg","accum30","vv30agg", "vvsd30agg","npol30agg","ndvi30yrRS", "ndwias30agg","nppmmid30agg","deficitRS",   "hdi30RS","cad30RS","decid30RS","cpg30precip", "pca_B1")]

responseRFprev <- rec_selJuicedprev$stream

```


*FFS 12th HUC Random Forest*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrl12rf <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices12sel$index)
#set up control for prevalence
ctrl12rfprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices12selprev$index)
#run with balanced
set.seed(1234)
ffs12rf <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrl12rf)

#run with prevalence
set.seed(1234)
ffs12rfprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrl12rfprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffs12rf, size =4, pch = 19,lwd = .1) + ggtitle("FFS 12th HUC Random Forest Balanced")
c2 <- plot_ffsCust(ffs12rfprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS 12th HUC Random Forest Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=17}
par(mfrow=c(1,2))
plot_ffsCust(ffs12rf,plotType = "selected", main = "FFS 12th HUC Random Forest Balanced",cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffs12rfprev,plotType = "selected", main = "FFS 12th HUC Random Forest Prevalence",cex.axis=1.5, cex.lab=1.5)
```



*FFS BlockCV Medium Random Forest*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrlMedrf <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesMedsel$index)
#set up control for prevalence
ctrlMedrfprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesMedselprev$index)
#run with balanced
set.seed(1234)
ffsMedrf <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrlMedrf)

#run with prevalence
set.seed(1234)
ffsMedrfprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrlMedrfprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffsMedrf, size =4, pch = 19,lwd = .1) + ggtitle("FFS BlockCV Medium Random Forest Balanced")
c2 <- plot_ffsCust(ffsMedrfprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS BlockCV Medium Random Forest Prevalence")

grid.arrange(c9,c10, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=20}
par(mfrow=c(1,2))
plot_ffsCust(ffsMedrf,plotType = "selected", main = "FFS BlockCV Medium Random Forest Balanced",cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffsMedrfprev,plotType = "selected", main = "FFS BlockCV Medium Random Forest Prevalence",cex.axis=1.5, cex.lab=1.5)
```

*FFS 14th HUC Random Forest*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrl14rf <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices14sel$index)
#set up control for prevalence
ctrl14rfprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices14selprev$index)
#run with balanced
set.seed(1234)
ffs14rf <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrl14rf)

#run with prevalence
set.seed(1234)
ffs14rfprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrl14rfprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```
```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffs14rf, size =4, pch = 19,lwd = .1) + ggtitle("FFS 14th HUC Random Forest Balanced")
c2 <- plot_ffsCust(ffs14rfprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS 14th HUC Random Forest Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=24}
par(mfrow=c(1,2))
plot_ffsCust(ffs14rf,plotType = "selected", main = "FFS 14th HUC Random Forest Balanced",cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffs14rfprev,plotType = "selected", main = "FFS 14th HUC Random Forest Prevalence",cex.axis=1.5, cex.lab=1.5)
```

*FFS Kmeans Random Forest*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrlKrf <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesKsel$index)
#set up control for prevalence
ctrlKprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesKselprev$index)
#run with balanced
set.seed(1234)
ffsKrf <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrlKrf)

#run with prevalence
set.seed(1234)
ffsKrfprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "rf",
  trControl = ctrlKprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```
```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffsKrf, size =4, pch = 19,lwd = .1) + ggtitle("FFS Kmeans Random Forest Balanced")
c2 <- plot_ffsCust(ffsKrfprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS Kmeans Random Forest Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=20}
par(mfrow=c(1,2))
plot_ffsCust(ffsKrf,plotType = "selected", main = "FFS Kmeans Random Forest Balanced",cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffsKrfprev,plotType = "selected", main = "FFS Kmeans Random Forest Prevalence",cex.axis=1.5, cex.lab=1.5)
```


#### FFS GBM

It won't matter if we use `predictorsRF` and `predictorRFprev` so if you notice be aware that it's fine.

*FFS 12th HUC GBM*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrl12g <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices12sel$index)
#set up control for prevalence
ctrl12gprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices12selprev$index)
#run with balanced
set.seed(1234)
ffs12g <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrl12g)

#run with prevalence
set.seed(1234)
ffs12gprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrl12gprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```
```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffs12g, size =4, pch = 19,lwd = .1) + ggtitle("FFS 12th HUC GBM Balanced")
c2 <- plot_ffsCust(ffs12gprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS 12th HUC GBM Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=19}
par(mfrow=c(1,2))
plot_ffsCust(ffs12g,plotType = "selected", main = "FFS 12th HUC GBM Balanced",cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffs12gprev,plotType = "selected", main = "FFS 12th HUC GBM Prevalence",cex.axis=1.5, cex.lab=1.5)
```

*FFS BlockCV Medium GBM*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrlMedg <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesMedsel$index)
#set up control for prevalence
ctrlMedgprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesMedselprev$index)
#run with balanced
set.seed(1234)
ffsMedg <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrlMedg)

#run with prevalence
set.seed(1234)
ffsMedgprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrlMedgprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```
```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffsMedg, size =4, pch = 19,lwd = .1) + ggtitle("FFS BlockCV Medium GBM Balanced")
c2 <- plot_ffsCust(ffsMedgprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS BlockCV Medium GBM Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=19}
par(mfrow=c(1,2))
plot_ffsCust(ffsMedg,plotType = "selected", main = "FFS BlockCV Medium GBM Balanced",cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffsMedgprev,plotType = "selected", main = "FFS BlockCV Medium GBM Prevalence",cex.axis=1.5, cex.lab=1.5)
```


*FFS 14th HUC GBM*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrl14g <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices14sel$index)
#set up control for prevalence
ctrl14gprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indices14selprev$index)
#run with balanced
set.seed(1234)
ffs14g <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrl14g)

#run with prevalence
set.seed(1234)
ffs14gprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrl14gprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```
```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffs14g, size =4, pch = 19,lwd = .1) + ggtitle("FFS 14th HUC GBM Balanced")
c2 <- plot_ffsCust(ffs14gprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS 14th HUC GBM Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=19}
par(mfrow=c(1,2))
plot_ffsCust(ffs14g,plotType = "selected", main = "FFS 14th HUC GBM Balanced", cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffs14gprev,plotType = "selected", main = "FFS 14th HUC GBM Prevalence", cex.axis=1.5, cex.lab=1.5)
```

*FFS Kmeans GBM*

```{r eval=FALSE}
#bring in cores
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

#set up control
ctrlKg <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesKsel$index)
#set up control for prevalence
ctrlKgprev <- trainControl(method="repeatedcv",
                     repeats = 5,
                     allowParallel = TRUE,
                     returnResamp = "all", 
                     verbose = FALSE, 
                     index = indicesKselprev$index)
#run with balanced
set.seed(1234)
ffsKg <- ffs(predictorsRF,responseRF, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrlKg)

#run with prevalence
set.seed(1234)
ffsKgprev <- ffs(predictorsRFprev,responseRFprev, 
  metric = "Kappa",
  method = "gbm",
  trControl = ctrlKgprev)


#now remove cluster. consistency...
stopCluster(cluster)
registerDoSEQ()

```
```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=8, fig.width=16}
#now plot
c1 <- plot_ffsCust(ffsKg, size =4, pch = 19,lwd = .1) + ggtitle("FFS 14th HUC GBM Balanced")
c2 <- plot_ffsCust(ffsKgprev,size = 4, pch = 19, lwd = .1) + ggtitle("FFS 14th HUC GBM Prevalence")

grid.arrange(c1,c2, ncol=2)
```

```{r echo=FALSE, eval=TRUE, message=FALSE, fig.align='center', fig.height=6, fig.width=19}
par(mfrow=c(1,2))
plot_ffsCust(ffsKg,plotType = "selected", main = "FFS 14th HUC GBM Balanced", cex.axis=1.5, cex.lab=1.5)

plot_ffsCust(ffsKgprev,plotType = "selected", main = "FFS 14th HUC GBM Prevalence", cex.axis=1.5, cex.lab=1.5)
```
## Model Performance 

#### Quick Intro

**Model Performance**

In this section we will now start to tune the selected variables from the feature selection process in the earlier section. This is where we will really try to optimize the performance using different techniques and approaches. There are different ways (advantages and disadvantages) to tuning certain model types. Random forest is pretty straight foward since it has only two parameters (e.g. mtry and ntrees) but GBM has multiple and can be sensitive to certain tuning schemes [@kuhn2013applied]. Therefore, it should be handled with care when exploring tuning options. Also, depending on what the objectives are for your project or study will ultimately direct your tuning and evaluation techniques. For more details on the 'why are we tuning?' go to the evaluation techniques section as that will give the rationale. This is just the nuts and bolts, so lots of graphs and code! 


#### RFE downsizing  


With rfe we end up with a lot of subsets (e.g. important variables), which we then need to decide how to filter or choose. Let us suppose we took a subset with the highest Kappa statistic, which contains 14 variables, but there were 5 other subsets with 4,5,6,7,10 variables within 2% of the Kappa value. Then it can be advantageous to take the one with the least amount of variables (e.g. curse of dimensionality) within a certain range (e.g. 1-5%). We could also look at the 'one standard error' approach by @breiman1984classification and take the least complex model (e.g. low variable or parameter) within one standard error of the highest performing metric. These all have their trade-offs and some approaches work better for different types of models or feature selection; however, it's prudent to explore these options and know the costs associated with them.  

The goal is to not fall into the trap of doing this for every model selection or feature selection process, e.g. feature selection vs. feature engineering. In our case we are using it to trim down the rfe feature selection process and not the final GBM or random forest models. Also, it wouldn't be appropriate to use with ffs, since ffs is essentially doing it for us. Thus if we use it for tuning (i.e. feature engineering) then we need to be careful because of different tuning parameters that are involved with the process. This is very subtle but should be highlighted. For example, how is a random forest model with 2 mtry and 1000 trees less complex than a model with 6 mtry and 250 trees? This is just with random forest, which only has two free parameters! This only gets more complicated with GBM, which has 4-5 free parameters and can lead to a very subjective selection process using the 'least complex' approach [@kuhn2013applied].  

Therefore, with rfe we will look at both the standard error and tolerance approach. Hence the highest performing feature selection might not be the model we pick and tune, i.e. this is the default function of rfe. If this is too confusing I appologize and it's my inability to express this concept (guilty as charged); however, what I'm trying to say is that by selecting the model based on performance (default settings) results should be handled carefully. Hopefully the descriptions below from the function manual will help explain this process more clearly or for more information go to [topepo](https://topepo.github.io/caret/model-training-and-tuning.html#choosing-the-final-model).  

"`oneSE` is a rule in the spirit of the "one standard error" rule of @breiman1984classification, who suggest that the tuning parameter associated with the best performance may over fit. They suggest that the simplest model within one standard error of the empirically optimal model is the better choice. This assumes that the models can be easily ordered from simplest to most complex (see the Details section below).

`tolerance` takes the simplest model that is within a percent tolerance of the empirically optimal model. For example, if the largest Kappa value is 0.5 and a simpler model within 3 percent is acceptable, we score the other models using (x - 0.5)/0.5 * 100. The simplest model whose score is not less than 3 is chosen (in this case, a model with a Kappa value of 0.35 is acceptable)."

`r tufte::quote_footer('--- Max Kuhn')`  

<br>

Here's a function below that will take different tolerance inputs and graph them so we can see how the rfe subsets react with certain tolerance values. This will hopefully give us an idea of some of the gains we can get from reducing dimensions while keeping performance relatively close to the maximium.  

```{r}

toltune <- function(object, len, metric = "", title = "", plot = TRUE){
  
  object <- object$results
  res = integer()
  for (i in 1:len){
    
    res[i] <- tolerance(object, metric = metric, tol = i,maximize = TRUE)  
    
  }
  tolit <- object[res,1:6]
  tolit$tolerance <- seq(1,len, 1)
  
  if (plot == "TRUE"){
    
  pp <-  tolit %>% ggplot(aes_string("tolerance",metric)) + 
      geom_point(aes(color = factor(Variables)), size = 3) + geom_line() + 
      ggtitle(paste(title),"statistic against tolerance values") + 
    scale_x_continuous(breaks = 1:len) +
  labs(color = "# Variables")
 
 
  
  } else {
    
  }
    
  
  return(list(tolit, pp))
  
}
```
<br>

Now we can look at the different rfe models compared to the tolerance ranges. Ideally we want to pick the least complex model and in our case this would be related to variable size. The basic function above helps us do this.  


```{r, echo=FALSE, fig.align='center',fig.width=9,fig.height=19}
z1 <- toltune(rfe12, 10, "Kappa", title = "RFE 12th HUC Random Forest")
z2 <- toltune(rfe12g, 10 , "Kappa", "RFE 12th HUC GBM")

z3 <- toltune(rfeMed, 10, "Kappa", title = "RFE BlockCV Med Random Forest")
z4 <- toltune(rfeMedg, 10 , "Kappa", "RFE BlockCV Med GBM")

z5 <- toltune(rfe14, 10, "Kappa", title = "RFE 14th HUC Random Forest")
z6 <- toltune(rfe14g, 10 , "Kappa", "RFE 14th HUC GBM")

z7 <- toltune(rfeK, 10, "Kappa", title = "RFE Kmeans Random Forest")
z8 <- toltune(rfeKg, 10 , "Kappa", "RFE Kmeans GBM")

z9 <- toltune(rfe12prev, 10, "Kappa", title = "RFEprev 12th HUC Random Forest")
z10 <- toltune(rfe12gprev, 10 , "Kappa", "RFEprev 12th HUC GBM")

z11 <- toltune(rfeMedprev, 10, "Kappa", title = "RFEprev BlockCV Med Random Forest")
z12 <- toltune(rfeMedgprev, 10 , "Kappa", "RFEprev BlockCV Med GBM")

z13 <- toltune(rfe14prev, 10, "Kappa", title = "RFEprev 14th HUC Random Forest")
z14 <- toltune(rfe14gprev, 10 , "Kappa", "RFEprev 14th HUC GBM")

z15 <- toltune(rfeKprev, 10, "Kappa", title = "RFEprev Kmeans Random Forest")
z16 <- toltune(rfeKgprev, 10 , "Kappa", "RFEprev Kmeans GBM")

grid.arrange(z1[[2]],z2[[2]],
             z9[[2]],z10[[2]],
             z3[[2]],z4[[2]],
             z11[[2]],z12[[2]],
             z5[[2]],z6[[2]],
             z13[[2]],z14[[2]],
             z7[[2]],z8[[2]],
             z15[[2]],z16[[2]],
             ncol=2)
```
<br>

As you can see it's nice to look at how these different variable subsets plot against a range of tolerances. This will hopefully give us a better idea of what subset to choose and how variables/models are reacting with rfe. It seems like the random forest model flattens out when decreasing in tolerance and GBM steadily decreases as you can see above. Not sure what to make of this yet, so comments or thoughts would be much appreciated. Now we can look at the one standard error approach.  

Let's make a function so we can handle all the models. Create a list with all the models and then we can iterate through them and save the `oneSD` result.


```{r}

oneStan <- data.frame()

oneReturn <- function(objlist, metric = "", num, maximize = TRUE){
  

  y <- NULL
 for (i in 1:length(objlist)){
  
  object <- objlist[[i]]
    
   object <- object$results
  
  a <- oneSE(object, metric = "Kappa", num = 10 , maximize = TRUE)
  
  a <- object[a,]
    y <- rbind(y, a)
    
  }
  return(y)
}

oneR <- oneReturn(objlist, "Kappa", num = 10)
oneR1 <- oneReturn(objlist2,"Kappa", num = 10)
```
<br>


We can now plot them to see how the add up. Not in order, sorry. 


```{r, fig.width=10, fig.align='center'}
oneR %>% ggplot(aes(model, Kappa, color = factor(model))) + geom_point(size = 5) + ggtitle("Balanced oneSD Kappa values per Model")
oneR1 %>% ggplot(aes(model, Kappa, color = factor(model))) + geom_point(size = 5) + ggtitle("Prevalence oneSD Kappa values per Model")

```
<br>

Now we can add the `tolerance` selection and see how they add up between the two methods. To choose the `tolerance` group I went with the closest to the highest performance, e.g. flat sections that still had high performance. Thus it was easy with random forest but GBM only lost a few variables, so I was a little more conservative. We can now see below that Both methods look pretty close to each other so I feel pretty comfortable using either the `tolerance` approach or the `oneR` approach. The `oneR` approach seems to be a lot easier and less descriptive so I'll most likely use that to get the final subset from rfe.

```{r, echo=FALSE, fig.width=10, fig.align='center'}
visOneTol <- data.frame(model = c("RFE RF12","RFE GBM12","RFE RFMed","RFE GBM Med","RFE RF14","RFE GBM 14","RFE RFK" ,"RFE GBM K"), 
               Kappa = c(.597,.586,.62,.61,.64,.65,.696,.67,.59,.56,.62,.61,.64,.63,.69,.67),
               method = c("tolerance","tolerance","tolerance","tolerance","tolerance","tolerance","tolerance","tolerance",
                          "oneR","oneR","oneR","oneR","oneR","oneR","oneR","oneR"))

visOneTol %>% ggplot(aes(model, Kappa, color = method)) + geom_jitter(size = 4,width = .25) + ggtitle("Balanced Data")

visOneTol1 <- data.frame(model = c("RFEprev GBM12","RFEprev RF12","RFEprev RFMed","RFEprev GBM Med","RFEprev RF14","RFEprev GBM 14","RFEprev RFK" ,"RFEprev GBM K"), 
               Kappa = c(.45,.43,.48,.485,.495,.52,.48,.50,.43,.43,.44,.485,.49,.516,.44,.486),
               method = c("tolerance","tolerance","tolerance","tolerance","tolerance","tolerance","tolerance","tolerance",
                          "oneR","oneR","oneR","oneR","oneR","oneR","oneR","oneR"))

visOneTol1 %>% ggplot(aes(model, Kappa, color = method)) + geom_jitter(size = 4,width = .25) + ggtitle("Prevalence Data")
```

<br>
 

#### RFE tuning Random Forest

Now that we've figured out which rfe subset we are going to use we can start tuning the parameters (e.g. trees, depth, etc). This tuning process will also include the `thresholder` function from `caret` to optimize the cutoff threshold in the response after we find the best model using Specificity.This is a similar approach to @hird2017google were AUC was used to tune the model and then Youdens J was used to find the optimal classifying threshold. However, we will be using Specificity and Youdens J, which should give us a model that can perform well on the naturally imbalanced data. 

```{r, echo=FALSE}
kableExtra::kable(oneR[,c(1,3,7)], caption = "Table of 'One Standard Error' Kappa Values per Model; Balanced") %>% kableExtra::kable_styling(full_width = TRUE, bootstrap_options = "striped")
kableExtra::kable(oneR1[,c(1,3,7)], caption = "Table of 'One Standard Error' Kappa Values per Model; Prevalence") %>% kableExtra::kable_styling(full_width = TRUE, bootstrap_options = "striped")
```


*RFE 12th HUC Random Forest Tune*
```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
rfe12$optVariables[1:5]
rfe12prev$optVariables[1:5]

levels(traintune$stream) <- c("X0","X1")

rec_tune12 <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-stream, new_role = "bring along")

rec_tune12prev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-stream, new_role = "bring along")

rec_tune12prevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-stream, new_role = "bring along")

customRF <- getModelInfo(model = "rf", regex = FALSE)[[1]]

#make custom parameters that we'll use on all random forest models
names(customRF)

prm <- data.frame(parameter = c("mtry", "ntree"),
                  class = rep("numeric",2),
                  label = c("#Random Predictors", "#of Trees"))

customRF$parameters <- prm

customRF$grid <- function(x, y, len = NULL, search = "grid") {
                    if(search == "grid") {
                      out <- data.frame(mtry = caret::var_seq(p = ncol(x), 
            classification = is.factor(y), len = len))
                    } else {
                      out <- data.frame(mtry = unique(sample(1:ncol(x), size = len, replace = TRUE)))
                    }
                  }

customRF$fit <- function(x, y, wts, param, lev, last, classProbs, ...){
                    randomForest::randomForest(x, y, mtry = param$mtry,ntree = param$ntree, ...)}

```

```{r eval=FALSE}

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfe12tune <- train(rec_tune12, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices12tune$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
rfe12tuneprev <- train(rec_tune12prev, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices12tuneprev$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
rfe12tuneprevDS <- train(rec_tune12prevDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices12tuneprev$index,
                                       summaryFunction = customRFsum))

stopCluster(cluster)
registerDoSEQ()

rfe12tuneThresh <- thresholder(rfe12tune, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
rfe12tuneprevThresh <- thresholder(rfe12tuneprev, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
rfe12tuneprevThreshDS <- thresholder(rfe12tuneprevDS, threshold = seq(0,1,.02)) #we'll use as threshold for classifying

```



```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- rfe12tuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(rfe12tune)
g00 <- plot(rfe12tuneprev)
confusionMatrix.train(rfe12tune,norm = "none")
confusionMatrix.train(rfe12tuneprev, norm = "none")
confusionMatrix.train(rfe12tuneprevDS, norm = "none")

g3 <- rfe12tune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- rfe12tune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- rfe12tuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- rfe12tuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- rfe12tune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- rfe12tuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- rfe12tuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfe12tuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")


grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)


```


*RFE Med BlockCV Random Forest Tune*

```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
library(caret)
library(CAST)
rfeMed$optVariables[1:5]
rfeMedprev$optVariables[1:4]

rec_tuneMed <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-stream, new_role = "bring along")

rec_tuneMedprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-stream, new_role = "bring along")

rec_tuneMedprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-stream, new_role = "bring along")


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfeMedtune <- train(rec_tuneMed, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesMedtune$index,
                                       summaryFunction = multiClassSummary))

set.seed(1234)
rfeMedtuneprev <- train(rec_tuneMedprev, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:3,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesMedtuneprev$index,
                                       summaryFunction = multiClassSummary))

set.seed(1234)
rfeMedtuneprevDS <- train(rec_tuneMedprevDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:3,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesMedtuneprev$index,
                                       summaryFunction = multiClassSummary))
stopCluster(cluster)
registerDoSEQ()

rfeMedtuneThresh <- thresholder(rfeMedtune, threshold = seq(0,1,.02))
rfeMedtuneprevThresh <- thresholder(rfeMedtuneprev, threshold = seq(0,1,.02))
rfeMedtuneprevThreshDS <- thresholder(rfeMedtuneprevDS, threshold = seq(0,1,.02))

```
```{r, echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- rfeMedtuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(rfeMedtune)
g00 <- plot(rfeMedtuneprev)
confusionMatrix(rfeMedtune,norm = "none")
confusionMatrix(rfeMedtuneprev, norm = "none")
confusionMatrix(rfeMedtuneprevDS, norm = "none")

g3 <- rfeMedtune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- rfeMedtune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- rfeMedtuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- rfeMedtuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- rfeMedtune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- rfeMedtuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- rfeMedtuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfeMedtuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")
grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)
```



*RFE 14th HUC Random Forest Tune*

```{r, eval=FALSE}

#get variable subset from oneSD results

rfe14$optVariables[1:5]
rfe14prev$optVariables[1:6]

rec_tune14 <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-stream, new_role = "bring along")

rec_tune14prev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-hdi30RS,-stream, new_role = "bring along")

rec_tune14prevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-hdi30RS,-stream, new_role = "bring along") %>% 
  step_downsample(stream)


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfe14tune <- train(rec_tune14, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices14tune$index,
                                       summaryFunction = multiClassSummary))

set.seed(1234)
rfe14tuneprev <- train(rec_tune14prev, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:5,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices14tuneprev$index,
                                       summaryFunction = multiClassSummary))


set.seed(1234)
rfe14tuneprevDS <- train(rec_tune14prevDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:5,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices14tuneprev$index,
                                       summaryFunction = multiClassSummary))
stopCluster(cluster)
registerDoSEQ()

rfe14tuneThresh <- thresholder(rfe14tune, threshold = seq(0,1,.02))
rfe14tuneprevThresh <- thresholder(rfe14tuneprev, threshold = seq(0,1,.02))
rfe14tuneprevThreshDS <- thresholder(rfe14tuneprevDS, threshold = seq(0,1,.02))


```
```{r, echo=FALSE, message=FALSE, fig.align='center',, fig.height=16, fig.width=13}
metrics <- rfe14tuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(rfe14tune)
g00 <- plot(rfe14tuneprev)
confusionMatrix.train(rfe14tune,norm = "none")
confusionMatrix.train(rfe14tuneprev, norm = "none")
confusionMatrix.train(rfe14tuneprevDS, norm = "none")


g3 <- rfe14tune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- rfe14tune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- rfe14tuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- rfe14tuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- rfe14tune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- rfe14tuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- rfe14tuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfe14tuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```


*RFE Kmeans Random Forest Tune*

```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
rfeK$optVariables

rec_tuneK <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-cad30RS,-stream, new_role = "bring along")

rec_tuneKprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-stream, new_role = "bring along")

rec_tuneKprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-cpg30precip,-stream, new_role = "bring along") %>%
  step_downsample(stream, skip = TRUE)
  
  
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfeKtune <- train(rec_tuneK, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesKtune$index,
                                       summaryFunction = multiClassSummary))

set.seed(1234)
rfeKtuneprev <- train(rec_tuneKprev, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:3,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesKtuneprev$index,
                                       summaryFunction = multiClassSummary))
set.seed(1234)
rfeKtuneprevDS <- train(rec_tuneKprevDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:3,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesKtuneprev$index,
                                       summaryFunction = multiClassSummary))

stopCluster(cluster)
registerDoSEQ()

rfeKtuneThresh <- thresholder(rfeKtune, threshold = seq(0,1,.02))
rfeKtuneprevThresh <- thresholder(rfeKtuneprev, threshold = seq(0,1,.02))
rfeKtuneprevThreshDS <- thresholder(rfeKtuneprevDS, threshold = seq(0,1,.02))


```
```{r, echo=FALSE, message=FALSE, fig.align='center',, fig.height=16, fig.width=13}
metrics <- rfeKtuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(rfeKtune)
g00 <- plot(rfeKtuneprev)
confusionMatrix.train(rfeKtune,norm = "none")
confusionMatrix.train(rfeKtuneprev, norm = "none")
confusionMatrix.train(rfeKtuneprevDS, norm = "none")


g3 <- rfeKtune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- rfeKtune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- rfeKtuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- rfeKtuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- rfeKtune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- rfeKtuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- rfeKtuneprevThresh[,c(3,4,5,16,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfeKtuneprevThreshDS[,c(3,4,5,16,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g89 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")
grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g89, ncol = 2)

```

#### RFE tuning GBM

*RFE 12th HUC GBM Tune*  

```{r, eval=FALSE}
#we'll use this tunegrid for all gbm models
gbmGrid <- expand.grid(interaction.depth = c(1,2,4,6,8), n.trees = c(500,1000,1500,2000,3000), shrinkage = c(0.001,.05,0.1,0.2,0.5), n.minobsinnode = c(10,15,25))
#get variable subset from oneSD results
oneR
oneR1
rfe12gprev$optVariables[1:7]
runi

rec_tune12g <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-nppmmid30agg,-stream, new_role = "bring along") 

rec_tune12gprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>%
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B",num_comp = 1)

rec_tune12gprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_downsample(stream) %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)
  
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfe12gtune <- train(rec_tune12g, data = traintune,
              method = "gbm", tuneLength = 20,
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices12tune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfe12gtuneprev <- train(rec_tune12gprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices12tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfe12gtuneprevDS <- train(rec_tune12gprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices12tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)
stopCluster(cluster)
registerDoSEQ()

rfe12gtuneThresh <- thresholder(rfe12gtune, threshold = seq(0,1,.02))
rfe12gtuneprevThresh <- thresholder(rfe12gtuneprev, threshold = seq(0,1,.02))
rfe12gtuneprevThreshDS <- thresholder(rfe12gtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- rfe12gtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(rfe12gtune,norm = "none")
confusionMatrix.train(rfe12gtuneprev, norm = "none")
confusionMatrix.train(rfe12gtuneprevDS, norm = "none")

g2 <- rfe12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- rfe12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- rfe12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- rfe12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- rfe12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- rfe12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- rfe12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- rfe12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- rfe12gtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfe12gtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```


*RFE blockCV Med GBM Tune*  

```{r, eval=FALSE}

#get variable subset from oneSD results
oneR
oneR1
rfeMedg$optVariables[1:6]
rfeMedgprev$optVariables[1:5]

library(recipes)

rec_tuneMedg <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-stream, new_role = "bring along") 

rec_tuneMedgprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9, num_comp = 1)
  
rec_tuneMedgprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_downsample(stream) %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9, num_comp = 1)

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfeMedgtune <- train(rec_tuneMedg, data = traintune,
              method = "gbm",
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesMedtune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfeMedgtuneprev <- train(rec_tuneMedgprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesMedtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfeMedgtuneprevDS <- train(rec_tuneMedgprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesMedtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

stopCluster(cluster)
registerDoSEQ()

rfeMedgtuneThresh <- thresholder(rfeMedgtune, threshold = seq(0,1,.02))
rfeMedgtuneprevThresh <- thresholder(rfeMedgtuneprev, threshold = seq(0,1,.02))
rfeMedgtuneprevThreshDS <- thresholder(rfeMedgtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- rfeMedgtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(rfeMedgtune,norm = "none")
confusionMatrix.train(rfeMedgtuneprev, norm = "none")
confusionMatrix.train(rfeMedgtuneprevDS, norm = "none")

g2 <- rfeMedgtune$results %>% ggplot() +geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- rfeMedgtuneprev$results %>% ggplot() +geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- rfeMedgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- rfeMedgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- rfeMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- rfeMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- rfeMedgtune$results %>% ggplot() +geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- rfeMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- rfeMedgtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfeMedgtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```


*RFE 14th HUC GBM Tune*  

```{r, eval=FALSE}

#get variable subset from oneSD results
oneR
oneR1
rfe14g$optVariables[1:6]
rfe14gprev$optVariables[1:13]

rec_tune14g <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-nppmmid30agg,-cpg30precip,-tpi30agg,-decid30RS,-cad30RS,-stream, new_role = "bring along") 
 
rec_tune14gprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-hdi30RS,-cad30RS,-vv30agg,-twi30agg,-ndwias30agg,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9, num_comp = 2)
  
rec_tune14gprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-hdi30RS,-cad30RS,-vv30agg,-twi30agg,-ndwias30agg,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_downsample(stream) %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9, num_comp = 2)


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfe14gtune <- train(rec_tune14g, data = traintune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices14tune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfe14gtuneprev <- train(rec_tune14gprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices14tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfe14gtuneprevDS <- train(rec_tune14gprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices14tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

stopCluster(cluster)
registerDoSEQ()

rfe14gtuneThresh <- thresholder(rfe14gtune, threshold = seq(0,1,.02))
rfe14gtuneprevThresh <- thresholder(rfe14gtuneprev, threshold = seq(0,1,.02))
rfe14gtuneprevThreshDS <- thresholder(rfe14gtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- rfe14gtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(rfe14gtune,norm = "none")
confusionMatrix.train(rfe14gtuneprev, norm = "none")
confusionMatrix.train(rfe14gtuneprevDS, norm = "none")

g2 <- rfe14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- rfe14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- rfe14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- rfe14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- rfe14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- rfe14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- rfe14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- rfe14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- rfe14gtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfe14gtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")
grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```

*RFE Kmeans GBM*

```{r, eval=FALSE}

#get variable subset from oneSD results
oneR
oneR1
rfeKg$optVariables[1:6]
rfeKgprev$optVariables[1:7]

rec_tuneKg <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-nppmmid30agg,-cpg30precip,-tpi30agg,-decid30RS,-cad30RS,-stream, new_role = "bring along") 
 
rec_tuneKgprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9, num_comp = 1)
  
rec_tuneKgprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-tpi30agg,-cpg30precip,-decid30RS,-deficitRS,-stream,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg, new_role = "bring along") %>% 
  step_downsample(stream) %>% 
  step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", threshold = 0.9, num_comp = 1)

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
rfeKgtune <- train(rec_tuneKg, data = traintune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesKtune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfeKgtuneprev <- train(rec_tuneKgprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesKtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
rfeKgtuneprevDS <- train(rec_tuneKgprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesKtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)
stopCluster(cluster)
registerDoSEQ()

rfeKgtuneThresh <- thresholder(rfeKgtune, threshold = seq(0,1,.02))
rfeKgtuneprevThresh <- thresholder(rfeKgtuneprev, threshold = seq(0,1,.02))
rfeKgtuneprevThreshDS <- thresholder(rfeKgtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- rfeKgtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(rfeKgtune,norm = "none")
confusionMatrix.train(rfeKgtuneprev, norm = "none")
confusionMatrix.train(rfeKgtuneprevDS, norm = "none")

g2 <- rfeKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- rfeKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- rfeKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- rfeKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- rfeKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- rfeKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- rfeKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- rfeKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- rfeKgtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- rfeKgtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```

#### RFE tuned partial dependency plots

Now that we've tuned both models (rf and gbm) we can start to look at some of the variable dependency plots. This will give us an idea of how the variables are reacting to the response and where the model seems to find it important. What will be interesting is how each structure (12th,14th,blockCV,Kmeans) reacts and also the between balanced and prevalence. We'll basically just plot all the pdp's by model type and structure type and variable. 


*Look at accum30 for random forest*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r1 <- partial(rfe12tune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC Random Forest Balanced")
r2 <- partial(rfe12tuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r3 <- partial(rfeMedtune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Balanced")
r4 <- partial(rfeMedtuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Prevalence")
r5 <- partial(rfe14tune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Balanced")
r6 <- partial(rfe14tuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r7 <- partial(rfeKtune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Balanced")
r8 <- partial(rfeKtuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r1, r3, r5, r7, 
             r2,r4,r6,r8,ncol = 4)  

```
*Look at accum30 for gbm*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r9 <- partial(rfe12gtune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM Balanced")
r10 <- partial(rfe12gtuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r11 <- partial(rfeMedgtune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r12 <- partial(rfeMedgtuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r13 <- partial(rfe14gtune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r14 <- partial(rfe14gtuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r15 <- partial(rfeKgtune, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r16 <- partial(rfeKgtuneprev, pred.var = "accum30", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r9, r11, r13, r15, 
             r10,r12,r14,r16,ncol = 4)  

```


*Look at nppmmid30agg for Random Forest*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r17 <- partial(rfe12tune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC Random Forest Balanced")
r18 <- partial(rfe12tuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r19 <- partial(rfeMedtune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Balanced")
r20 <- partial(rfeMedtuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Prevalence")
r21 <- partial(rfe14tune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Balanced")
r22 <- partial(rfe14tuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r23 <- partial(rfeKtune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Balanced")
r24 <- partial(rfeKtuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r17, r19, r21, r23, 
             r18,r20,r22,r24,ncol = 4)  

```


*Look at nppmmid30agg for GBM*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r25 <- partial(rfe12gtune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM Balanced")
r26 <- partial(rfe12gtuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r27 <- partial(rfeMedgtune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r28 <- partial(rfeMedgtuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r29 <- partial(rfe14gtune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r30 <- partial(rfe14gtuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r31 <- partial(rfeKgtune, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r32 <- partial(rfeKgtuneprev, pred.var = "nppmmid30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r25, r27, r29, r31, 
             r26,r28,r30,r32,ncol = 4)  

```


*Look at tpi30agg for Random Forest*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)
oneR1
r33 <- partial(rfe12tune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC Random Forest Balanced")
r34 <- partial(rfe12tuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r35 <- partial(rfeMedtune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Balanced")
r36 <- partial(rfeMedtuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Prevalence")
r37 <- partial(rfe14tune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Balanced")
r38 <- partial(rfe14tuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r39 <- partial(rfeKtune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Balanced")
r40 <- partial(rfeKtuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r33, r35, r37, r39, 
             r34,r36,r38,r40,ncol = 4)  

```


*Look at tpi30agg for GBM*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r41 <- partial(rfe12gtune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM Balanced")
r42 <- partial(rfe12gtuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r43 <- partial(rfeMedgtune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r44 <- partial(rfeMedgtuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r45 <- partial(rfe14gtune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r46 <- partial(rfe14gtuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r47 <- partial(rfeKgtune, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r48 <- partial(rfeKgtuneprev, pred.var = "tpi30agg", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r41, r43, r45, r47, 
             r42,r44,r46,r48,ncol = 4)  

```

*Look at cpg30precip for Random Forest*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r49 <- partial(rfe12tune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC Random Forest Balanced")
r50 <- partial(rfe12tuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r51 <- partial(rfeMedtune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Balanced")
r52 <- partial(rfeMedtuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Prevalence")
r53 <- partial(rfe14tune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Balanced")
r54 <- partial(rfe14tuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r55 <- partial(rfeKtune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Balanced")
r56 <- partial(rfeKtuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r49, r51, r53, r55, 
             r50,r52,r54,r56,ncol = 4)  

```

*Look at cpg30precip for GBM*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r57 <- partial(rfe12gtune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM Balanced")
r58 <- partial(rfe12gtuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r59 <- partial(rfeMedgtune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r60 <- partial(rfeMedgtuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r61 <- partial(rfe14gtune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r62 <- partial(rfe14gtuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r63 <- partial(rfeKgtune, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r64 <- partial(rfeKgtuneprev, pred.var = "cpg30precip", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r57, r59, r61, r63, 
             r58,r60,r62,r64,ncol = 4)  

```



*Look at cad30RS for Random Forest*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)
r65 <- partial(rfe12tune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC Random Forest Balanced")
r66 <- partial(rfe12tuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r67 <- partial(rfeMedtune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Balanced")
r68 <- partial(rfeMedtuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium Random Forest Prevalence")
r69 <- partial(rfe14tune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Balanced")
r70 <- partial(rfe14tuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC Random Forest Prevalence")
r71 <- partial(rfeKtune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Balanced")
r72 <- partial(rfeKtuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans Random Forest Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r65, r67, r69, r71, 
             r66,r68,r70,r72,ncol = 4)  

```




*Look at cad30RS for GBM*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)

r73 <- partial(rfe12gtune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM Balanced")
r74 <- partial(rfe12gtuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r75 <- partial(rfeMedgtune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r76 <- partial(rfeMedgtuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r77 <- partial(rfe14gtune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r78 <- partial(rfe14gtuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r79 <- partial(rfeKgtune, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r80 <- partial(rfeKgtuneprev, pred.var = "cad30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r73, r75, r77, r79, 
             r74,r76,r78,r80,ncol = 4)  
```



Now the variables are getting scarce between models and structures. However, GBM did have more diversity than random forest that included `deficitRS` and `decid30RS`. We will look at those below.

*Look at decid30RS for GBM*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)
r81 <- partial(rfe12gtune, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM  Balanced")
r82 <- partial(rfe12gtuneprev, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r83 <- partial(rfeMedgtune, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r84 <- partial(rfeMedgtuneprev, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r85 <- partial(rfe14gtune, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r86 <- partial(rfe14gtuneprev, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r87 <- partial(rfeKgtune, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r88 <- partial(rfeKgtuneprev, pred.var = "decid30RS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r81, r83, r85, r87, 
             r82,r84,r86,r88,ncol = 4)  

```


*Look at deficitRS for GBM*
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(vip)
rfe14gprev$optVariables

r89 <- partial(rfe12gtune, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 12th HUC GBM Balanced")
r90 <- partial(rfe12gtuneprev, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r91 <- partial(rfeMedgtune, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Balanced")
r92 <- partial(rfeMedgtuneprev, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE BlockCV Medium GBM Prevalence")
r93 <- partial(rfe14gtune, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Balanced")
r94 <- partial(rfe14gtuneprev, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE 14th HUC GBM Prevalence")
r95 <- partial(rfeKgtune, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = traintune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Balanced")
r96 <- partial(rfeKgtuneprev, pred.var = "deficitRS", trim.outliers = TRUE, prob = TRUE) %>% plotPartial(train = trainprevtune, rug=TRUE,smooth=TRUE, main="RFE Kmeans GBM Prevalence")

```
```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(r89, r91, r93, r95, 
             r90,r92,r94,r96,ncol = 4)  
```



```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd <- partial(rfe12tune, pred.var = c("cpg30precip","nppmmid30agg"), prob = TRUE)
pd1 <- plotPartial(pd,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Balance",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd2 <- plotPartial(pd, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE,screen = list(z = -60, x = -60), alpha = 0.5, main = "RFE 12th HUC Random Forest Balance",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd1,pd2, ncol = 2)
```
```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd3 <- partial(rfe12tuneprev, pred.var = c("nppmmid30agg","cpg30precip"), prob = TRUE)
pd4 <- plotPartial(pd3,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Prev",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd5 <- plotPartial(pd3, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE,screen = list(z = -60, x = -60), alpha = 0.5, main = "RFE 12th HUC Random Forest Prev",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd4,pd5, ncol = 2)
```


```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd6 <- partial(rfe12tune, pred.var = c("nppmmid30agg","accum30"), prob = TRUE, trim.outliers = TRUE)
pd7 <- plotPartial(pd6,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd8 <- plotPartial(pd6, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5,,screen = list(z = -45, x = -70, y = -5), main = "RFE 12th HUC Random Forest balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd7,pd8, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd9 <- partial(rfe12tune, pred.var = c("nppmmid30agg","tpi30agg"), prob = TRUE)
pd10 <- plotPartial(pd9,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd11 <- plotPartial(pd9, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 12th HUC Random Forest balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd10,pd11, ncol = 2)
```


```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd12 <- partial(rfe12tune, pred.var = c("nppmmid30agg","cad30RS"), prob = TRUE)
pd13 <- plotPartial(pd12,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd14 <- plotPartial(pd12, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 12th HUC Random Forest balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd13,pd14, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd15 <- partial(rfe12tune, pred.var = c("nppmmid30agg","cpg30precip","tpi30agg"), prob = TRUE)
pd16 <- plotPartial(pd15,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd17 <- plotPartial(pd15, levelplot = FALSE, zlab = "tpi30agg",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 12th HUC Random Forest balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=19, fig.width=15}
grid.arrange(pd16,pd17, ncol = 1)
```

<br>
After looking at the interactions between these variables there are only a few interactions that I want to look at that include prevalence data. The reason is that most of the pdp's are reacting the same regardless of balanced or prevalence and only the probability (yhat) is shifted/different. So, from here out will mostly just be balanced data between random forest, gbm, and dependence structures. 


```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd18 <- partial(rfe12gtune, pred.var = c("nppmmid30agg","accum30"), prob = TRUE, trim.outliers = TRUE)
pd19 <- plotPartial(pd18,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd20 <- plotPartial(pd18, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 12th HUC GBM balanced",screen = list(z = 0, x = -80, y = 30),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd19,pd20, ncol = 2)
```
```{r, eval=FALSE, echo=FALSE}
library(pdp)
pd21 <- partial(rfeMedtune, pred.var = c("nppmmid30agg","cpg30precip"), prob = TRUE)
pd22 <- plotPartial(pd21,colorkey = TRUE,contour = TRUE, main = "RFE BlockCV Medium Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd23 <- plotPartial(pd21, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE BlockCV Medium Random Forest balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd22,pd23, ncol = 2)
```




```{r, eval=FALSE, echo=FALSE}
library(pdp)

pd27 <- partial(rfe14tune, pred.var = c("nppmmid30agg","cpg30precip"), prob = TRUE)
pd28 <- plotPartial(pd27,colorkey = TRUE,contour = TRUE, main = "RFE 14th HUC Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd29 <- plotPartial(pd27, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 14th HUC Random Forest balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd28,pd29, ncol = 2)
```
```{r, eval=FALSE, echo=FALSE}
library(pdp)

pd24 <- pdp::partial(rfeKtune, pred.var = c("nppmmid30agg","cpg30precip"), prob = TRUE)
pd25 <- plotPartial(pd24,colorkey = TRUE,contour = TRUE, main = "RFE Kmeans Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd26 <- plotPartial(pd24, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE Kmeans Random Forest balanced",screen = list(z = -60, x = -60),col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd25,pd26, ncol = 2)
```
```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd30 <- pdp::partial(object = rfe12tune, pred.var = c("nppmmid30agg","cad30RS","cpg30precip"), prob = TRUE)
pd31 <- plotPartial(pd30,colorkey = TRUE,contour = TRUE, main = "RFE 12th HUC Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd32 <- plotPartial(pd30, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 12th HUC Random Forest balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=19, fig.width=15}
grid.arrange(pd31,pd32, ncol = 1)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd33 <- pdp::partial(object = rfeMedgtune, pred.var = c("deficitRS","decid30RS"), prob = TRUE)
pd34 <- plotPartial(pd33,colorkey = TRUE,contour = TRUE, main = "RFE BlockCV GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd35 <- plotPartial(pd33, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE BlockCV GBM balanced",screen = list(z = 40, x = -60, y =0),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd34,pd35, ncol = 2)
```


```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd33 <- pdp::partial(object = rfeMedgtune, pred.var = c("cpg30precip","deficitRS"), prob = TRUE)
pd34 <- plotPartial(pd33,colorkey = TRUE,contour = TRUE, main = "RFE BlockCV GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd35 <- plotPartial(pd33, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE BlockCV GBM balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd34,pd35, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd36 <- pdp::partial(object = rfe14gtune, pred.var = c("cpg30precip","decid30RS"), prob = TRUE)
pd37 <- plotPartial(pd36,colorkey = TRUE,contour = TRUE, main = "RFE 14th HUC GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd38 <- plotPartial(pd36, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 14th HUC GBM balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd37,pd38, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd39 <- pdp::partial(object = rfeMedtune, pred.var = c("cad30RS","nppmmid30agg","accum30"), prob = TRUE, trim.outliers = TRUE)
pd40 <- plotPartial(pd39,colorkey = TRUE,contour = TRUE, main = "RFE BlockCV Medium Random Forest Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd41 <- plotPartial(pd39, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE BlockCV Medium Random Forest balanced",screen = list(z = -60, x = -60),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=19, fig.width=15}
grid.arrange(pd40,pd41, ncol = 1)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd42 <- pdp::partial(object = rfeMedgtune, pred.var = c("nppmmid30agg","accum30","deficitRS"), prob = TRUE, trim.outliers = TRUE)
pd43 <- plotPartial(pd42,colorkey = TRUE,contour = TRUE, main = "RFE BlockCV Medium GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd44 <- plotPartial(pd42, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE BlockCV Medium GBM balanced",screen = list(z = 0, x = -80, y = 30),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=19, fig.width=15}
grid.arrange(pd43,pd44, ncol = 1)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd45 <- pdp::partial(object = rfeMedgtune, pred.var = c("nppmmid30agg","deficitRS"), prob = TRUE)
pd46 <- plotPartial(pd45,colorkey = TRUE,contour = TRUE, main = "RFE BlockCV Medium GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd47 <- plotPartial(pd45, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE BlockCV Medium GBM balanced",screen = list(z = -60, x = -80),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd46,pd47, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd48 <- pdp::partial(object = rfe14gtune, pred.var = c("nppmmid30agg","decid30RS"), prob = TRUE)
pd49 <- plotPartial(pd48,colorkey = TRUE,contour = TRUE, main = "RFE 14th HUC GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd50 <- plotPartial(pd48, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 14th HUC GBM balanced",screen = list(z = -60, x = -55),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd49,pd50, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd51 <- pdp::partial(object = rfe14tune, pred.var = c("cad30RS","cpg30precip"), prob = TRUE)
pd52 <- plotPartial(pd51,colorkey = TRUE,contour = TRUE, main = "RFE 14th HUC GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd53 <- plotPartial(pd51, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 14th HUC GBM balanced",screen = list(z = 20, x = -50, y=10),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd52,pd53, ncol = 2)
```

```{r, eval=FALSE, echo=FALSE}
library(pdp)
library(randomForest)
library(gbm)
pd54 <- pdp::partial(object = rfeMedgtune, pred.var = c("decid30RS","deficitRS"), prob = TRUE)
pd55 <- plotPartial(pd54,colorkey = TRUE,contour = TRUE, main = "RFE 14th HUC GBM Balanced",
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
pd56 <- plotPartial(pd54, levelplot = FALSE, zlab = "yhat",drape=TRUE,
colorkey = TRUE,contour = TRUE, alpha = 0.5, main = "RFE 14th HUC GBM balanced",screen = list(z = -60, x = -55),
col.regions = colorRampPalette(c("blue", "yellow","red"))(100))
```

```{r echo=FALSE,eval=TRUE,message=FALSE, fig.align='center', fig.height=8, fig.width=20}
grid.arrange(pd55,pd56, ncol = 2)
```


### FFS tuned Random Forest

Just use the same tuning features as with RFE. 

*FFS 12th HUC Random Forest Tune*
```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
ffs12rf$selectedvars
ffs12rfprev$selectedvars

"accum30"      "decid30RS"    "tpi30agg"     "nppmmid30agg" "deficitRS"    "twi30agg"     "ndvi30yrRS" 
"accum30"     "deficitRS"   "cpg30precip" "hdi30RS"     "vvsd30agg"
levels(traintune$stream) <- c("X0","X1")

rec_tune12ffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-decid30RS,-deficitRS,-twi30agg,-ndvi30yrRS,-stream, new_role = "bring along")

rec_tune12prevffs <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-cpg30precip,-vvsd30agg,-hdi30RS,-deficitRS,-stream, new_role = "bring along")
  

rec_tune12prevffsDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-cpg30precip,-vvsd30agg,-hdi30RS,-deficitRS,-stream, new_role = "bring along")


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffs12tune <- train(rec_tune12ffs, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices12tune$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffs12tuneprev <- train(rec_tune12prevffs, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices12tuneprev$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffs12tuneprevDS <- train(rec_tune12prevffsDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:4,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices12tuneprev$index,
                                       summaryFunction = customRFsum))

stopCluster(cluster)
registerDoSEQ()
ffs12tune$bestTune
ffs12tuneThresh <- thresholder(ffs12tune, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffs12tuneprevThresh <- thresholder(ffs12tuneprev, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffs12tuneprevThreshDS <- thresholder(ffs12tuneprevDS, threshold = seq(0,1,.02)) #we'll use as threshold for classifying

```

```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffs12tuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(ffs12tune)
g00 <- plot(ffs12tuneprev)
confusionMatrix.train(ffs12tune,norm = "none")
confusionMatrix.train(ffs12tuneprev, norm = "none")
confusionMatrix.train(ffs12tuneprevDS, norm = "none")

g3 <- ffs12tune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- ffs12tune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- ffs12tuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- ffs12tuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- ffs12tune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- ffs12tuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- ffs12tuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffs12tuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")


grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)


```

*FFS BlockCV Medium Random Forest Tune*
```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
ffsMedrf$selectedvars
ffsMedrfprev$selectedvars

levels(traintune$stream) <- c("X0","X1")

"accum30"      "decid30RS"    "nppmmid30agg" "cad30RS"      "tpi30agg"     "deficitRS" 
"accum30"      "cpg30precip"  "nppmmid30agg" "cad30RS"      "pca_B1"       "hdi30RS"      "vv30agg"  

rec_tuneMedffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-decid30RS,-cad30RS,-deficitRS,-stream, new_role = "bring along")

rec_tuneMedprevffs <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-hdi30RS,-nppmmid30agg,-cpg30precip,-cad30RS,-vv30agg, -B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg, -stream, new_role = "bring along") %>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tuneMedprevffsDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-hdi30RS,-nppmmid30agg,-cpg30precip,-cad30RS,-vv30agg, -B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg,-stream, new_role = "bring along") %>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffsMedtune <- train(rec_tuneMedffs, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:5,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesMedtune$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffsMedtuneprev <- train(rec_tuneMedprevffs, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesMedtuneprev$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffsMedtuneprevDS <- train(rec_tuneMedprevffsDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesMedtuneprev$index,
                                       summaryFunction = customRFsum))

stopCluster(cluster)
registerDoSEQ()

ffsMedtuneThresh <- thresholder(ffsMedtune, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffsMedtuneprevThresh <- thresholder(ffsMedtuneprev, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffsMedtuneprevThreshDS <- thresholder(ffsMedtuneprevDS, threshold = seq(0,1,.02)) #we'll use as threshold for classifying

```

```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffsMedtuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(ffsMedtune)
g00 <- plot(ffsMedtuneprev)
confusionMatrix.train(ffsMedtune,norm = "none")
confusionMatrix.train(ffsMedtuneprev, norm = "none")
confusionMatrix.train(ffsMedtuneprevDS, norm = "none")

g3 <- ffsMedtune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- ffsMedtune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- ffsMedtuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- ffsMedtuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- ffsMedtune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- ffsMedtuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- ffsMedtuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffsMedtuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")


grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

ffsMedtuneprev$bestTune
```

```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
ffs14rf$selectedvars
ffs14rfprev$selectedvars

levels(traintune$stream) <- c("X0","X1")

  "accum30"      "decid30RS"    "nppmmid30agg" "ndvi30yrRS"   "tpi30agg"     "cad30RS"      "deficitRS"    "twi30agg" 
  "accum30"      "cpg30precip"  "nppmmid30agg" "cad30RS"      "pca_B1"       "hdi30RS"      "vv30agg" 

rec_tune14ffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-tpi30agg,-nppmmid30agg,-decid30RS,-cad30RS,-deficitRS,-ndvi30yrRS,-deficitRS,-twi30agg,-stream, new_role = "bring along")

rec_tune14prevffs <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-hdi30RS,-nppmmid30agg,-cpg30precip,-cad30RS,-vv30agg, -B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg, -stream, new_role = "bring along") %>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tune14prevffsDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-hdi30RS,-nppmmid30agg,-cpg30precip,-cad30RS,-vv30agg, -B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg,-stream, new_role = "bring along") %>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffs14tune <- train(rec_tune14ffs, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:7,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices14tune$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffs14tuneprev <- train(rec_tune14prevffs, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices14tuneprev$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffs14tuneprevDS <- train(rec_tune14prevffsDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indices14tuneprev$index,
                                       summaryFunction = customRFsum))

stopCluster(cluster)
registerDoSEQ()

ffs14tuneThresh <- thresholder(ffs14tune, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffs14tuneprevThresh <- thresholder(ffs14tuneprev, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffs14tuneprevThreshDS <- thresholder(ffs14tuneprevDS, threshold = seq(0,1,.02)) #we'll use as threshold for classifying

```

```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffs14tuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(ffs14tune)
g00 <- plot(ffs14tuneprev)
confusionMatrix.train(ffs14tune,norm = "none")
confusionMatrix.train(ffs14tuneprev, norm = "none")
confusionMatrix.train(ffs14tuneprevDS, norm = "none")

g3 <- ffs14tune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- ffs14tune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- ffs14tuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- ffs14tuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- ffs14tune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- ffs14tuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- ffs14tuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffs14tuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")


grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)


```


*FFS Kmeans Random Forest*
```{r, eval=FALSE}

#get variable subset from oneSD results
library(recipes)
ffsKrf$selectedvars
ffsKrfprev$selectedvars

levels(traintune$stream) <- c("X0","X1")

 "accum30"      "nppmmid30agg" "cpg30precip"  "cad30RS"      "pca_B1"       "deficitRS"    "ndvi30yrRS"  
 
 "accum30"      "cpg30precip"  "nppmmid30agg" "cad30RS"      "pca_B1"       "hdi30RS"      "vv30agg"

rec_tuneKffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-cpg30precip,-nppmmid30agg,-deficitRS,-cad30RS,-ndvi30yrRS,-B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg,-stream, new_role = "bring along")%>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tuneKprevffs <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-hdi30RS,-nppmmid30agg,-cpg30precip,-cad30RS,-vv30agg, -B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg, -stream, new_role = "bring along") %>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tuneKprevffsDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
  update_role(sb12,sb14,sbMed,spatial_cluster, HUC12, HUC14, coords.x1,coords.x2, new_role = "bring along") %>% 
  update_role(-accum30,-hdi30RS,-nppmmid30agg,-cpg30precip,-cad30RS,-vv30agg, -B2_30agg,-B3_30agg,-B4_30agg, -B8_30agg,-stream, new_role = "bring along") %>% 
  step_center(contains("_30agg")) %>% 
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)


cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffsKtune <- train(rec_tuneKffs, data = traintune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesKtune$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffsKtuneprev <- train(rec_tuneKprevffs, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesKtuneprev$index,
                                       summaryFunction = customRFsum))

set.seed(1234)
ffsKtuneprevDS <- train(rec_tuneKprevffsDS, data = trainprevtune,
              method = customRF,
              metric = "AUC",
              tuneGrid = expand.grid(mtry = 1:6,ntree=seq(100,1500,200)),
              trControl = trainControl(method = "repeatedcv",
                                       repeats = 5,
                                       classProbs = TRUE,
                                       savePredictions = 'all',
                                       index = indicesKtuneprev$index,
                                       summaryFunction = customRFsum))

stopCluster(cluster)
registerDoSEQ()

ffsKtuneThresh <- thresholder(ffsKtune, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffsKtuneprevThresh <- thresholder(ffsKtuneprev, threshold = seq(0,1,.02)) #we'll use as threshold for classifying
ffsKtuneprevThreshDS <- thresholder(ffsKtuneprevDS, threshold = seq(0,1,.02)) #we'll use as threshold for classifying

```

```{r echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffsKtuneThresh[,c(3,4,5,17)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")
g2 <- plot(ffsKtune)
g00 <- plot(ffsKtuneprev)
confusionMatrix.train(ffsKtune,norm = "none")
confusionMatrix.train(ffsKtuneprev, norm = "none")
confusionMatrix.train(ffsKtuneprevDS, norm = "none")

g3 <- ffsKtune$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry))) + ggtitle("Balanced")
g4 <- ffsKtune$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g5 <- ffsKtuneprev$results %>% ggplot(aes(ntree, Specificity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g6 <- ffsKtuneprev$results %>% ggplot(aes(ntree, Sensitivity)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")
g111 <- ffsKtune$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Balanced")
g222 <- ffsKtuneprev$results %>% ggplot(aes(ntree, Kappa)) + geom_point(aes(color=factor(mtry))) + geom_line(aes(color=factor(mtry)))+ ggtitle("Prevalence")

met <- ffsKtuneprevThresh[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffsKtuneprevThreshDS[,c(3,4,5,17)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")


grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)


```

#### FFS tuning GBM

*FFS 12th HUC GBM Tune*  

```{r, eval=FALSE}
#we'll use this tunegrid for all gbm models
gbmGrid <- expand.grid(interaction.depth = c(1,2,4,6,8), n.trees = c(500,1000,1500,2000,3000), shrinkage = c(0.001,.05,0.1,0.2,0.5), n.minobsinnode = c(10,15,25))
#get variable names from ffs

ffs12g$finalModel$var.names
ffs12gprev$finalModel$var.names

"accum30"   "deficitRS" "cad30RS"   "pca_B1"
"accum30"   "deficitRS" "twi30agg"  "cad30RS"  

rec_tune12gffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-deficitRS,-cad30RS,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg,-stream, new_role = "bring along") %>% 
step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tune12gffsprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-cad30RS,-twi30agg,-deficitRS,-stream, new_role = "bring along") 

rec_tune12gffsprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
 update_role(-accum30,-cad30RS,-twi30agg,-deficitRS,-stream, new_role = "bring along") %>% 
  step_downsample(stream)
  
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffs12gtune <- train(rec_tune12gffs, data = traintune,
              method = "gbm",
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices12tune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffs12gtuneprev <- train(rec_tune12gffsprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices12tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffs12gtuneprevDS <- train(rec_tune12gffsprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices12tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)
stopCluster(cluster)
registerDoSEQ()

ffs12gtuneThresh <- thresholder(ffs12gtune, threshold = seq(0,1,.02))
ffs12gtuneprevThresh <- thresholder(ffs12gtuneprev, threshold = seq(0,1,.02))
ffs12gtuneprevThreshDS <- thresholder(ffs12gtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffs12gtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(ffs12gtune,norm = "none")
confusionMatrix.train(ffs12gtuneprev, norm = "none")
confusionMatrix.train(ffs12gtuneprevDS, norm = "none")

g2 <- ffs12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- ffs12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- ffs12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- ffs12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- ffs12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- ffs12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- ffs12gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- ffs12gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- ffs12gtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffs12gtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```

*FFS BlockCV Medium GBM Tune*  

```{r, eval=FALSE}
#we'll use this tunegrid for all gbm models
gbmGrid <- expand.grid(interaction.depth = c(1,2,4,6,8), n.trees = c(500,1000,1500,2000,3000), shrinkage = c(0.001,.05,0.1,0.2,0.5), n.minobsinnode = c(10,15,25))
#get variable names from ffs

ffsMedg$finalModel$var.names
ffsMedgprev$finalModel$var.names

"accum30"      "deficitRS"    "tpi30agg"     "nppmmid30agg" "pca_B1"       "decid30RS"    "twi30agg" 
"accum30"      "nppmmid30agg" "twi30agg"    

rec_tuneMedgffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-deficitRS,-tpi30agg,-nppmmid30agg,-decid30RS,-twi30agg,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg,-stream, new_role = "bring along") %>% 
step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tuneMedgffsprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-twi30agg,-stream, new_role = "bring along") 

rec_tuneMedgffsprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
  step_downsample(stream) %>% 
 update_role(-accum30,-nppmmid30agg,-twi30agg,-stream, new_role = "bring along")
  
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffsMedgtune <- train(rec_tuneMedgffs, data = traintune,
              method = "gbm",
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesMedtune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffsMedgtuneprev <- train(rec_tuneMedgffsprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesMedtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffsMedgtuneprevDS <- train(rec_tuneMedgffsprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesMedtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)
stopCluster(cluster)
registerDoSEQ()

ffsMedgtuneThresh <- thresholder(ffsMedgtune, threshold = seq(0,1,.02))
ffsMedgtuneprevThresh <- thresholder(ffsMedgtuneprev, threshold = seq(0,1,.02))
ffsMedgtuneprevThreshDS <- thresholder(ffsMedgtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffsMedgtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(ffsMedgtune,norm = "none")
confusionMatrix.train(ffsMedgtuneprev, norm = "none")
confusionMatrix.train(ffsMedgtuneprevDS, norm = "none")

g2 <- ffsMedgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- ffsMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- ffsMedgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- ffsMedgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- ffsMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- ffsMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- ffsMedgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- ffsMedgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- ffsMedgtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffsMedgtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```


*FFS 14th HUC GBM Tune*  

```{r, eval=FALSE}
#we'll use this tunegrid for all gbm models
gbmGrid <- expand.grid(interaction.depth = c(1,2,4,6,8), n.trees = c(500,1000,1500,2000,3000), shrinkage = c(0.001,.05,0.1,0.2,0.5), n.minobsinnode = c(10,15,25))
#get variable names from ffs

ffs14g$finalModel$var.names
ffs14gprev$finalModel$var.names

"accum30"      "deficitRS"    "nppmmid30agg" "tpi30agg"     "pca_B1" 
"accum30"      "nppmmid30agg" "twi30agg" 

rec_tune14gffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-deficitRS,-nppmmid30agg,-tpi30agg,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg,-stream, new_role = "bring along") %>% 
step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tune14gffsprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-nppmmid30agg,-twi30agg,-stream, new_role = "bring along") 

rec_tune14gffsprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
 update_role(-accum30,-nppmmid30agg,-twi30agg,-stream, new_role = "bring along") %>% 
  step_downsample(stream)
  
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffs14gtune <- train(rec_tune14gffs, data = traintune,
              method = "gbm",
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices14tune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffs14gtuneprev <- train(rec_tune14gffsprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices14tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffs14gtuneprevDS <- train(rec_tune14gffsprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indices14tuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)
stopCluster(cluster)
registerDoSEQ()

ffs14gtuneThresh <- thresholder(ffs14gtune, threshold = seq(0,1,.02))
ffs14gtuneprevThresh <- thresholder(ffs14gtuneprev, threshold = seq(0,1,.02))
ffs14gtuneprevThreshDS <- thresholder(ffs14gtuneprevDS, threshold = seq(0,1,.02))

```

```{r,echo=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffs14gtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(ffs14gtune,norm = "none")
confusionMatrix.train(ffs14gtuneprev, norm = "none")
confusionMatrix.train(ffs14gtuneprevDS, norm = "none")

g2 <- ffs14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- ffs14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- ffs14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- ffs14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- ffs14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- ffs14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- ffs14gtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- ffs14gtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- ffs14gtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffs14gtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```

*FFS Kmeans GBM Tune*  

```{r, eval=FALSE}
#we'll use this tunegrid for all gbm models
gbmGrid <- expand.grid(interaction.depth = c(1,2,4,6,8), n.trees = c(500,1000,1500,2000,3000), shrinkage = c(0.001,.05,0.1,0.2,0.5), n.minobsinnode = c(10,15,25))
#get variable names from ffs

ffsKg$finalModel$var.names
ffsKgprev$finalModel$var.names

"accum30"   "deficitRS" "cad30RS"   "pca_B1"
"accum30"   "deficitRS" "twi30agg"  "cad30RS"  

rec_tuneKgffs <-   recipe(stream ~ ., data = traintune) %>% 
  update_role(-accum30,-deficitRS,-cad30RS,-B2_30agg,-B3_30agg,-B4_30agg,-B8_30agg,-stream, new_role = "bring along") %>% 
step_center(contains("_30agg"))  %>%
  step_scale(contains("_30agg")) %>% 
  step_pca(contains("_30agg"), prefix = "pca_B", num_comp = 1)

rec_tuneKgffsprev <-   recipe(stream ~ ., data = trainprevtune) %>% 
  update_role(-accum30,-cad30RS,-twi30agg,-deficitRS,-stream, new_role = "bring along") 

rec_tuneKgffsprevDS <-   recipe(stream ~ ., data = trainprevtune) %>% 
 update_role(-accum30,-cad30RS,-twi30agg,-deficitRS,-stream, new_role = "bring along")
  
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

set.seed(1234)
ffsKgtune <- train(rec_tuneKgffs, data = traintune,
              method = "gbm",
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesKtune$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffsKgtuneprev <- train(rec_tuneKgffsprev, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesKtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)

set.seed(1234)
ffsKgtuneprevDS <- train(rec_tuneKgffsprevDS, data = trainprevtune,
              method = "gbm", 
                 metric = "AUC", distribution = "bernoulli",
                  trControl = trainControl(method = "repeatedcv",
                                           repeats = 5,
                                           classProbs = TRUE,
                                           savePredictions = 'all',
                                           index = indicesKtuneprev$index,
                                           summaryFunction = multiClassSummary),
              tuneGrid = gbmGrid)
stopCluster(cluster)
registerDoSEQ()

ffsKgtuneThresh <- thresholder(ffsKgtune, threshold = seq(0,1,.02))
ffsKgtuneprevThresh <- thresholder(ffsKgtuneprev, threshold = seq(0,1,.02))
ffsKgtuneprevThreshDS <- thresholder(ffsKgtuneprevDS, threshold = seq(0,1,.02))
```

```{r,echo=FALSE,eval=FALSE, message=FALSE, fig.align='center', fig.height=16, fig.width=13}
metrics <- ffsKgtuneThresh[,c(5,6,7,19)]
metrics <- reshape2::melt(metrics, id.vars = "prob_threshold", 
                variable.name = "Resampled",
                value.name = "Data")

g1 <- ggplot(metrics, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() + 
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Balanced")

confusionMatrix.train(ffsKgtune,norm = "none")
confusionMatrix.train(ffsKgtuneprev, norm = "none")
confusionMatrix.train(ffsKgtuneprevDS, norm = "none")

g2 <- ffsKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Balanced")
g00 <- ffsKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Specificity, color = factor(n.trees)))+ ggtitle("Prevalence")

g3 <- ffsKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees))) + ggtitle("Balanced")
g4 <- ffsKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Balanced")
g5 <- ffsKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), AUC, color = factor(n.trees)))+ ggtitle("Prevalence")
g6 <- ffsKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Sensitivity, color = factor(n.trees)))+ ggtitle("Prevalence")
g111 <- ffsKgtune$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Balanced")
g222 <- ffsKgtuneprev$results %>% ggplot() + geom_boxplot(aes(factor(shrinkage), Kappa, color = factor(n.trees)))+ ggtitle("Prevalence")

met <- ffsKgtuneprevThresh[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g0 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence")

met <- ffsKgtuneprevThreshDS[,c(5,6,7,19)]
met <- reshape2::melt(met, id.vars = "prob_threshold", 
                          variable.name = "Resampled",
                          value.name = "Data")

g000 <- ggplot(met, aes(x = prob_threshold, y = Data, color = Resampled)) + 
  geom_line() +
  ylab("") + xlab("Probability Cutoff") +
  theme(legend.position = "top") + ggtitle("Prevalence Downsampled")

grid.arrange(g1,g0,g2,g00,g3,g5,g4, g6,g111,g222,g000, ncol = 2)

```

## Model Assessment

Model Assessment

lsakdjfsalkdjf
as;ldfkjasldfj

</div>

# Discussion/Results




# References

**Bibliography** 

