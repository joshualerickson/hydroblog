---
title: 'ANOVA'
layout: single-sidebar
date: '2021-08-17'
slug: anova/2021-08-17-
categories:
  - R
  - Hydrology
  - Statistics
subtitle: 'A look into ANOVA. The long way.'
summary: 'A look into ANOVA. The long way.'
authors: []
lastmod: ''
featured: yes
output:
  blogdown::html_page:
    css: "style.css"
image:
  caption: '[Image credit: Josh Erickson](featured.png)'
  placement: 2
  focal_point: 'Center'
  preview_only: no
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>
<script src="{{< blogdown/postref >}}index_files/kePrint/kePrint.js"></script>
<link href="{{< blogdown/postref >}}index_files/lightable/lightable.css" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" type="text/css" />


<div id="intro" class="section level2">
<h2>Intro</h2>
<p>In this blog post we will dive into ANOVA aka Analysis of Variance and see how we derive the final output from a function like <code>anova()</code> by using packages like {tidyverse}. ANOVA is a OG when it comes to statistics and comparing multiple groups of data. It can seem weird at first but since it goes hand-in-hand with regression I think it makes it less scary. In this blog post we will go over one-way ANOVA. We want to start with some groups of data like wind-speed at multiple stations or mean annual flows for August at multiple gauging stations. Once we have this we can start performing ANOVA.</p>
</div>
<div id="eda" class="section level2">
<h2>EDA</h2>
<p>Let‚Äôs bring in some data and explore! This data is from three different weather stations and the random variable is wind speed. This is a perfect time to use the <code>tribble()</code> from which we can create a tibble. We‚Äôll also pivot the locations so that we can use for plotting and grouping.</p>
<pre class="r"><code>library(tidyverse)
wind_speed &lt;- tribble(
  ~loc_1, ~loc_2, ~loc_3, ~month,
  2.21,0.87,1.89, 1,
  0.62, 1.65, 3.03, 2,
  2.03, 0.74, 1.85, 3,
  0.8, 3.52, -0.29, 4,
  0.84, 2.27, 0.68, 5,
  1.52, 2.15, 2.76, 6,
  0.57, 1.33, 1.03, 7,
  1.39, 1.87, 0.88, 8,
  2.3, 1.93, 1.03, 9,
  1.78, 2.48, 2.49, 10,
  2.17, 1.44, 0.88, 11,
  1.72, 1.03, 1.17, 12
)

wind_speed_piv &lt;- wind_speed %&gt;% pivot_longer(cols = contains(&#39;loc&#39;))</code></pre>
<p>Now if we plot these groups out we can see visually what they look like together.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="960" /></p>
<p>I mean they seem similar? This is where the eye-ball test fails big time (even for seemingly different boxplots i.e.¬†95% CI don‚Äôt overlap). Welcome ANOVA. ANOVA comes in and helps us determine if any of these groups are different at some threshold. In this example we‚Äôll set the threshold to the OG 0.05 thresh. Remember, a p value is the probability of showing a difference equal to or more extreme than what we have observed! This means that we will be surprised if it is less than 0.05 but not so much surprised if it‚Äôs 0.5. But first, let‚Äôs make a better plot to also show the distribution of these groups alongside the boxplots.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="960" /></p>
<p>Ok, this looks much better (IMO) and more intuitive! Now I‚Äôm unsure if there isn‚Äôt a difference üòÑ. Why don‚Äôt we just find out and see. Like, just take the function <code>anova()</code> and see? Well, what fun would that be! Let‚Äôs keep the suspense going‚Ä¶</p>
</div>
<div id="assumptions" class="section level2">
<h2>Assumptions</h2>
<p>There are some assumptions when performing ANOVA that are very similar to regression üòâ.</p>
<ul>
<li>Normality within the groups distribution.</li>
<li>Independent and identically distributed (iid) assumption.</li>
<li>Groups of similar variance.</li>
</ul>
<p>To do this we‚Äôll test these assumptions with a few functions in the stats package. For normality, we‚Äôll use the <code>shapiro.test()</code> to test for normality with a 0.05 threshold. For iid, we‚Äôll assume that the stations are far enough away from each other so that they are not spatially autocorrelated and we‚Äôll assume the measurements are the same protocol. For variance, we‚Äôll use Bartlett‚Äôs test <code>bartlett.test</code> for similar variance across groups since the groups are normally distributed.</p>
<div id="results" class="section level4">
<h4>Results</h4>
<p>We are looking for all the p values to be above 0.05. If so, then we can move on in this example. If not, then we would need to do some data transformations or some different test.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
name
</th>
<th style="text-align:right;">
p.value
</th>
<th style="text-align:left;">
method
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
loc_1
</td>
<td style="text-align:right;">
0.1526072
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
loc_2
</td>
<td style="text-align:right;">
0.6533819
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
loc_3
</td>
<td style="text-align:right;">
0.5656978
</td>
<td style="text-align:left;">
Shapiro-Wilk normality test
</td>
</tr>
<tr>
<td style="text-align:left;">
all groups
</td>
<td style="text-align:right;">
0.4170791
</td>
<td style="text-align:left;">
Bartlett test of homogeneity of variances
</td>
</tr>
</tbody>
</table>
<p>In our case it looks like we are fine to move on and perform ANOVA! Remember this is a toy example and in the ‚Äòreal world‚Äô it is likely to not be so cookie-cutter but that‚Äôs fine! Don‚Äôt get shook! There are plenty of ways to correct for this, e.g.¬†transformations, different tests (allow for non-parametric data), etc.</p>
</div>
</div>
<div id="anova" class="section level2">
<h2>ANOVA</h2>
<p>So to perform ANOVA we need to step back and remember what we are trying to find, ‚Äòare there differences (variance) between these groups?‚Äô. To do that it would make sense to take the means of each group and the standard deviation and see how different they are right? Well let‚Äôs do that.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-6-1.png" width="960" /></p>
<p>Well, looks like there really isn‚Äôt any difference! But what does that look like mathematically? That is say, what the heck is ANOVA trying to do? Because sometimes it‚Äôs not as easy as the toy example we are looking at and we will eventually need to understand how to do it mathematically or report it instead of saying ‚Äòwell it looks the same‚Äô. Well ANOVA wants to take the groups and see if there variance is equal to each other, right? To do that, it needs to find out the variance due to attributes (SSA) and the variance due to random error (SSE). There has to be something with <em>variance</em>, right? I mean it is named analysis of <strong>variance</strong>‚Ä¶ So, mathematically it can be expressed in the formula below where SST is the total sum of squares,</p>
<p><span class="math display">\[
\text{SST} = \text{SSA} + \text{SSE} \\ 
\sum_{i=1}^{a}\sum_{j=i}^{n_i}(x_{ij}-\mu)^2=\sum_{i=1}^{a}\sum_{j=1}^{n_i}(\bar{x_i}-\mu)^2+\sum_{i=1}^{a}\sum_{j=1}^{n_i}(x_{ij}-\bar{x_i})^2
\]</span></p>
<p>This looks really familiar, no? That‚Äôs because the parts are really similar to regression methods, e.g.¬†the total sum of squares <span class="math inline">\(TSS = \sum_{i=1}^{n}(y_{i}-\bar{y})^2\)</span>. Remember we wanted to find out ‚Äúthe variance due to attributes (SSA) and the variance due to random error (SSE)‚Äù and that results in the total sum of squares for the samples. If there is a group that has more variance than the others attribute error (SSA) we should then pick that up when we compare it to the random error (SSE). Another way to think about this is graphically. Let‚Äôs look at the SSE in a graph. The image below shows the <span class="math inline">\(\sum_{j=1}^{n_i}(x_{ij}-\bar{x_i})^2\)</span> part of the equation, which shows the residuals (red lines) and then squaring and summing; however, we do this for each group and then sum! That then gives us the sum of squared errors SSE.</p>
<p><img src="resids.png" width="640" /></p>
<p>To calculate the SSA we just take the mean of each group minus the overall mean then square and sum. But, we then multiply by the number of points for <strong>one</strong> group. So in our example it would be <span class="math inline">\(12*\sum_{j=1}^{n_i}(\bar{x_i}-\mu)^2\)</span>. Ok, so how can we interpret all of this? For me, if a group is further away from the overall mean then it will increase SSA and thus influence the overall variance but might not always increase random error! This is important because the next step in ANOVA is to solve for the mean square error (MSE) and the mean square attribute (MSA), which involve ratios and can be sensitive to increases in one variable, e.g.¬†SSA. This is the whole point of ANOVA; figure out if there is more attribute error within the group compared to the groups random error. In the equation below <span class="math inline">\(N\)</span> is equal to the number of observations and <span class="math inline">\(a\)</span> is equal to the number of attributes/groups. In our case, <span class="math inline">\(N=33\)</span> &amp; <span class="math inline">\(a=3\)</span>.</p>
<p><span class="math display">\[
\text{MSE} = \frac{SSE}{N-a} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\text{MSA} = \frac{SSA}{a-1}
\]</span></p>
<p>Once we have these solved we can find the F statistic (from which we‚Äôll get the p value) by taking the ratio of <span class="math inline">\(MSE\)</span> and <span class="math inline">\(MSA\)</span>, e.g.¬†<span class="math inline">\(F=\frac{MSA}{MSE}\)</span>. If our F statistic is less than the critical value F statistic for a <span class="math inline">\(\alpha = 0.05\)</span> than we cannot reject the null hypothesis!</p>
</div>
<div id="finally" class="section level2">
<h2>Finally</h2>
<p>Ok, so we took a long time to get here but now we‚Äôll actually go ahead and solve it with our data step-by-step.</p>
<p>We‚Äôll need the overall mean and the mean of each group.</p>
<pre class="r"><code>overall_mean &lt;- wind_speed_piv %&gt;% 
  summarise(overall_mean = mean(value))

mean_of_each &lt;- wind_speed_piv %&gt;% 
  group_by(name) %&gt;% summarise(mean_of_each = mean(value)) %&gt;% ungroup</code></pre>
<p>Now we need to solve for SSA and SSE.</p>
<pre class="r"><code>ssa &lt;- wind_speed_piv %&gt;% mutate(overall_mean = overall_mean) %&gt;% 
  left_join(mean_of_each, by = &#39;name&#39;) %&gt;% 
  group_by(name)  %&gt;% slice_head(n = 1) %&gt;% ungroup() %&gt;% 
  summarise(ssa = 12*sum((mean_of_each - overall_mean)^2))

sse &lt;- wind_speed_piv %&gt;% mutate(overall_mean = overall_mean) %&gt;% 
  left_join(mean_of_each, by = &#39;name&#39;) %&gt;% 
  group_by(name) %&gt;% summarise(sse = sum((value - mean_of_each)^2)) %&gt;% 
  pull(sse) %&gt;% sum()</code></pre>
<p>Now we can solve for the MSA and MSE and thus the F statistic!</p>
<pre class="r"><code>MSA = ssa/2
MSE = sse/33

f = MSA/MSE</code></pre>
<p>Our final F statistic is 0.5596. If <span class="math inline">\(F&lt;F_{(0.05)}(2,33)\)</span> then we cannot reject the null hypothesis. <span class="math inline">\(F_{(0.05)}(2,33)=3.284\)</span> which means <span class="math inline">\(F&lt;F_{(0.05)}(2,33)\)</span> is true and we cannot reject the null! There is no statistical difference between groups at a 0.05 threshold! You can use the <code>qf()</code> to find the F critical value, e.g.¬†<code>qf(0.95,2,33)</code>.</p>
</div>
<div id="smart-easy-way" class="section level2">
<h2>Smart easy way</h2>
<p>The smart easy way, which is the way you will always do it most likely, is to take the linear model of the data and then use the <code>anova()</code>. We‚Äôll use this method and compare to what we got above.</p>
<pre class="r"><code>model &lt;- lm(value~name, data = wind_speed_piv)

anova(model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: value
##           Df  Sum Sq Mean Sq F value Pr(&gt;F)
## name       2  0.7346 0.36730  0.5596 0.5767
## Residuals 33 21.6590 0.65633</code></pre>
<p>Voila, the same! The F value is the same as above and the p value is greater than 0.05 so we cannot reject the null!</p>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>Hopefully this helps with understanding ANOVA! I try to look at it by how certain variables are more sensitive than others (SSA) as well as the similarities to regression, e.g.¬†explaining the variance in the data. If you have any comments, concerns or questions please let me know. Thanks.</p>
</div>
